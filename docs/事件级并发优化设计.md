# 事件级并发优化设计

## 当前架构分析

### 现有并发模型
```
用户需求
  ↓
分解为类别 [体育, 政治, 科技]
  ↓
┌─────────────────────────────────────────────┐
│  体育类别任务线（独立执行）                    │
│    ├─ 搜索体育新闻                            │
│    ├─ 聚合为事件 [事件1, 事件2, 事件3]       │
│    ├─ 处理事件1（验证+时间轴+预测）  ❌串行    │
│    ├─ 处理事件2（验证+时间轴+预测）  ❌串行    │
│    ├─ 处理事件3（验证+时间轴+预测）  ❌串行    │
│    └─ 生成报告                                │
└─────────────────────────────────────────────┘
```

**问题**:
- 类别级别已经并发,但类别内部的事件处理是串行的
- 如果体育类别有10个事件,需要逐个处理,耗时长

## 优化后的架构

### 完全并发模型
```
用户需求
  ↓
分解为类别 [体育, 政治, 科技]
  ↓
┌─────────────────────────────────────────────────────────────┐
│  体育类别任务线（完全并发）                                   │
│    ├─ 搜索体育新闻                                           │
│    ├─ 聚合为事件 [事件1, 事件2, 事件3]                      │
│    ├─ ┌─────────────────────────────────────────┐           │
│    │   │  事件级并发处理层 ⭐                     │           │
│    │   ├─────────────────────────────────────────┤           │
│    │   │                                         │           │
│    │   │  事件1处理任务 ─┐                        │           │
│    │   │  ├─ @validator  │ (独立并发执行)        │           │
│    │   │  ├─ @timeline-builder                  │           │
│    │   │  └─ @predictor                        │           │
│    │   │                                         │           │
│    │   │  事件2处理任务 ─┼→ 3个事件完全并行       │           │
│    │   │  ├─ @validator  │                       │           │
│    │   │  ├─ @timeline-builder                  │           │
│    │   │  └─ @predictor                        │           │
│    │   │                                         │           │
│    │   │  事件3处理任务 ─┘                        │           │
│    │   │    ├─ @validator                       │           │
│    │   │    ├─ @timeline-builder                │           │
│    │   │    └─ @predictor                       │           │
│    │   │                                         │           │
│    │   └─────────────────────────────────────────┘           │
│    │                                                         │
│    ├─ 等待所有事件处理完成                                   │
│    └─ @synthesizer 生成报告                                   │
└─────────────────────────────────────────────────────────────┘
```

**优势**:
- ✅ 类别级别并发: 体育、政治、科技同时处理
- ✅ 事件级别并发: 每个类别内的所有事件同时处理
- ✅ 分析步骤并发: 每个事件的验证、时间轴、预测同时执行
- ✅ **总耗时 = max(最慢类别的事件处理耗时)**,而非所有事件耗时之和

## 实现方案

### 方案1: 为每个事件启动独立Task (推荐)

在news-collector的主prompt中,修改事件处理逻辑:

```python
# 聚合事件后
events = ["事件1", "事件2", "事件3"]

# 为每个事件启动独立的完整处理任务(并发)
event_tasks = []
for event in events:
    task = Task(
        description=f"处理{category}类别的事件:{event}",
        prompt=f"""
        处理事件"{event}"的完整分析流程:
        1. 从数据库读取该事件的所有新闻
        2. 并行执行以下三个分析:
           - @validator 验证事件真实性
           - @timeline-builder 构建时间轴
           - @predictor 预测发展趋势
        3. 返回分析结果
        """,
        agent=news-collector  # 使用相同的agent,但专注于单个事件
    )
    event_tasks.append(task)

# 等待所有事件任务完成
# 收集所有结果
# @synthesizer 生成报告
```

### 方案2: 创建专用的事件处理agent

创建一个新的agent专门处理单个事件:

```json
"event-processor": {
  "mode": "subagent",
  "description": "事件处理器 - 并发处理单个事件的验证、时间轴、预测",
  "prompt": "{file:.opencode/agents/event-processor.md}",
  "hidden": true,
  "temperature": 0.2,
  "maxSteps": 25,
  "tools": {
    "write": false,
    "edit": false,
    "bash": false,
    "web-browser_*": true,
    "news-storage*": true
  }
}
```

event-processor的prompt内容:
```markdown
你是单个事件的并发处理专家。

任务: 对单个事件进行完整的分析,包括验证、时间轴构建、趋势预测。

工作流程:
1. 从数据库读取该事件的所有新闻
2. 并发启动三个分析任务(使用Task工具):
   - validator任务
   - timeline-builder任务
   - predictor任务
3. 等待三个任务完成
4. 返回综合结果
```

## 性能对比

### 场景: 3个类别,每个类别5个事件

**当前架构(串行处理事件)**:
```
总耗时 = 单个事件处理时间 × 事件数量 × 类别数量(取最大)
       = 5分钟 × 5个事件
       = 25分钟/类别
       = 25分钟(因为类别并发)
```

**优化后架构(并发处理事件)**:
```
总耗时 = 单个事件处理时间 × 分析步骤数量(因为步骤也并发)
       = 5分钟 × 1(三个分析同时进行)
       = 5分钟/类别
       = 5分钟(因为类别并发)
```

**性能提升**: **5倍加速** (25分钟 → 5分钟)

## 实现细节

### 1. 修改 news-collector prompt

在 [prompts/news-collector.txt](prompts/news-collector.txt) 中,将第103-105行:

```markdown
3. 对每个体育事件：
   - @validator 验证事件（从数据库读取该事件的所有新闻）
   - @timeline-builder 构建时间轴（从数据库读取）
   - @predictor 预测发展（从数据库读取）
```

修改为:

```markdown
3. 对每个体育事件,启动独立的并发处理任务:
   为每个事件创建独立的Task,该Task内部并发执行:
   - @validator 验证事件（从数据库读取该事件的所有新闻）
   - @timeline-builder 构建时间轴（从数据库读取）
   - @predictor 预测发展（从数据库读取）
```

### 2. 执行方式伪代码

```python
# 主流程
categories = ["体育", "政治", "科技"]

# 为每个类别启动独立任务(并发)
for category in categories:
    Task(
        description=f"处理{category}类别",
        prompt=f"""
        1. 搜索{category}新闻 → 保存到数据库
        2. @event-aggregator 聚合为事件
        3. 为每个事件启动独立的并发处理Task:

           # 事件级并发
           for event in events:
               Task(
                   description=f"处理事件:{event}",
                   prompt=f"""
                   并发执行以下三个分析:
                   - @validator(event_name="{event}")
                   - @timeline-builder(event_name="{event}")
                   - @predictor(event_name="{event}")
                   """
               )

        4. 等待所有事件Task完成
        5. @synthesizer 生成报告
        """
    )
```

### 3. 数据一致性保证

**关键点**:
- ✅ 每个事件独立处理,互不干扰
- ✅ 所有分析结果都保存到数据库
- ✅ synthesizer从数据库读取所有结果生成报告
- ✅ 使用event_name作为数据隔离标识

**数据库操作**:
```python
# validator、timeline-builder、predictor 都使用 event_name 查询
news-storage_search_news_tool(event_name="事件名称", limit=100)

# 各自保存分析结果时,也使用 event_name 标识
# synthesizer 通过 event_name 聚合所有分析结果
```

## 注意事项

### 1. 资源限制
- 并发Task数量 = 类别数量 × 每个类别的事件数量
- 如果3个类别,每个类别10个事件 = 30个并发Task
- 需要确保系统资源足够

### 2. 错误处理
- 单个事件失败不应影响其他事件
- 需要在最终报告中标注哪些事件处理失败

### 3. 结果聚合
- 所有事件处理完成后,需要收集结果
- synthesizer需要能够读取所有分析结果

### 4. 数据库性能
- 多个agent同时读写数据库
- 需要确保数据库支持并发访问(SQLite支持,但性能有限)

## 实施步骤

1. ✅ **创建event-processor agent** (可选)
2. ✅ **修改news-collector prompt** - 核心修改
3. ✅ **测试单个类别的事件并发**
4. ✅ **测试多个类别的事件并发**
5. ✅ **性能测试和优化**

## 推荐方案

**推荐使用方案1**: 直接在news-collector中为每个事件创建独立Task,不需要新增agent。

**理由**:
- 减少复杂度
- 复用现有的validator、timeline-builder、predictor
- 更灵活的控制粒度
- 更容易调试

## 示例代码

### news-collector的事件处理部分

```python
# 第2步:聚合事件后
events_result = event-aggregator()  # 返回事件列表
events = events_result["events"]  # ["事件1", "事件2", "事件3"]

# 第3步:为每个事件启动独立的并发处理任务
event_tasks = []
for event in events:
    task = Task(
        description=f"处理{category}类别的事件:{event['name']}",
        prompt=f"""
        处理事件"{event['name']}"的完整分析:

        1. 从数据库读取该事件的所有新闻:
           news-storage_search_news_tool(event_name="{event['name']}", limit=100)

        2. 并发执行以下三个分析:

           # 分析1: 验证真实性
           validation_result = @validator 验证事件"{event['name']}"

           # 分析2: 构建时间轴
           timeline_result = @timeline-builder 构建事件"{event['name']}"的时间轴

           # 分析3: 预测趋势
           prediction_result = @predictor 预测事件"{event['name']}"的发展趋势

        3. 返回综合分析结果:
           {{
             "event_name": "{event['name']}",
             "validation": validation_result,
             "timeline": timeline_result,
             "prediction": prediction_result
           }}
        """,
        agent=news-collector
    )
    event_tasks.append(task)

# 第4步:等待所有事件任务完成
# (系统会自动等待所有Task完成)

# 第5步:收集结果并生成报告
all_event_results = [task.result for task in event_tasks]
@synthesizer 生成报告(all_event_results)
```

## 总结

**优化目标**: 将事件级串行处理改为并发处理

**核心改动**:
1. 在聚合事件后,为每个事件创建独立的Task
2. 每个事件Task内部并发执行validator、timeline-builder、predictor
3. 等待所有事件Task完成后,synthesizer生成报告

**预期收益**:
- 对于N个事件,理论上可以获得N倍加速
- 加上类别级别的并发,整体性能提升可达 **类别数 × 事件数** 倍
- 实际收益受限于系统资源和数据库性能

**下一步行动**:
1. 修改 [prompts/news-collector.txt](prompts/news-collector.txt)
2. 测试验证
3. 性能测试
