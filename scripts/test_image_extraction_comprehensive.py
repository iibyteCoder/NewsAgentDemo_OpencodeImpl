"""
æµ‹è¯•å›¾ç‰‡é“¾æ¥æå–åŠŸèƒ½ - å…¨é¢æµ‹è¯•é›†

æµ‹è¯•ç›®æ ‡ï¼š
1. éªŒè¯ä¼˜åŒ–åçš„å›¾ç‰‡æå–ç®—æ³•èƒ½å¦å‡†ç¡®æå–æ­£æ–‡å›¾ç‰‡
2. ç¡®ä¿ä¸æå–é¡µçœ‰ã€é¡µè„šã€ä¾§è¾¹æ ã€å¹¿å‘Šç­‰æ— å…³åŒºåŸŸçš„å›¾ç‰‡
3. æµ‹è¯•å¤šç§ç½‘ç«™ç±»å‹ï¼ˆç§‘æŠ€æ–°é—»ã€ä½“è‚²æ–°é—»ã€è´¢ç»æ–°é—»ç­‰ï¼‰

ç”¨æ³•ï¼š
- åœ¨ VS Code ä¸­å³é”® -> "åœ¨ç»ˆç«¯ä¸­è¿è¡Œ Python æ–‡ä»¶" æˆ–æŒ‰ F5 ç›´æ¥è°ƒè¯•
- ç‚¹å‡»å³ä¸Šè§’çš„è¿è¡ŒæŒ‰é’®
"""

# æ ‡å‡†åº“å¯¼å…¥
import asyncio
import json
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import urllib.parse

# è®¾ç½®æ§åˆ¶å°ç¼–ç ä¸º UTF-8ï¼ˆé¿å… Windows GBK ç¼–ç é—®é¢˜ï¼‰
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", errors="replace")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# é¡¹ç›®å†…éƒ¨å¯¼å…¥
from mcp_server.web_browser.tools.search_tools import fetch_article_content
from mcp_server.downloader.tools.download_tools import download_files


# ==================== æµ‹è¯•å‡½æ•° ====================

async def test_single_url(
    url: str,
    title: str,
    max_display_length: int = 70,
    download_images: bool = False,
    image_save_dir: Optional[Path] = None
) -> Dict:
    """æµ‹è¯•å•ä¸ªURLçš„å›¾ç‰‡æå–åŠŸèƒ½

    Args:
        url: æ–°é—»URL
        title: æ–°é—»æ ‡é¢˜

    Returns:
        æµ‹è¯•ç»“æœå­—å…¸
    """
    print(f"\n{'=' * 60}")
    print(f"[æµ‹è¯•] {title}")
    print(f"URL: {url}")
    print("=" * 60)

    try:
        # è°ƒç”¨ fetch_article_content è·å–æ–‡ç« å†…å®¹å’Œå›¾ç‰‡é“¾æ¥
        print("  æ­£åœ¨è·å–æ–‡ç« å†…å®¹å’Œå›¾ç‰‡...")
        result_json = await fetch_article_content(url, include_images=True)

        # è§£æç»“æœ
        result = json.loads(result_json)

        # æå–å…³é”®ä¿¡æ¯
        article_title = result.get("title", "æ— æ ‡é¢˜")
        content_length = result.get("content_length", 0)
        images = result.get("images", [])
        image_count = result.get("image_count", len(images))
        status = result.get("status", {})

        # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        print(f"\n  [åŸºæœ¬ä¿¡æ¯]")
        print(
            f"    æ–‡ç« æ ‡é¢˜: {article_title[:50]}..."
            if len(article_title) > 50
            else f"    æ–‡ç« æ ‡é¢˜: {article_title}"
        )
        print(f"    æ­£æ–‡é•¿åº¦: {content_length} å­—ç¬¦")
        print(f"    é¡µé¢çŠ¶æ€: {status.get('status', 'unknown')}")
        print(f"    å›¾ç‰‡æ•°é‡: {image_count}")

        # æ˜¾ç¤ºå›¾ç‰‡åˆ—è¡¨
        download_results = []
        if images:
            print(f"\n  [å›¾ç‰‡åˆ—è¡¨] (å…± {len(images)} å¼ )")
            for i, img in enumerate(images, 1):
                img_url = img.get("url", "N/A")
                alt = img.get("alt", "")
                width = img.get("width", 0)
                height = img.get("height", 0)

                # æˆªæ–­è¿‡é•¿çš„URL
                display_url = (
                    img_url
                    if len(img_url) <= max_display_length
                    else img_url[: max_display_length - 3] + "..."
                )
                size_info = f" ({width}x{height})" if width and height else ""
                alt_info = f" - {alt}" if alt else ""

                print(f"    {i}. {display_url}{size_info}{alt_info}")

            # ä¸‹è½½å›¾ç‰‡
            if download_images and images:
                print(f"\n  [ä¸‹è½½å›¾ç‰‡] æ­£åœ¨ä¸‹è½½ {len(images)} å¼ å›¾ç‰‡...")
                try:
                    # åˆ›å»ºä¿å­˜ç›®å½•
                    save_dir = image_save_dir / _sanitize_filename(title) if image_save_dir else None
                    if save_dir:
                        save_dir.mkdir(parents=True, exist_ok=True)

                    # æå–å›¾ç‰‡URLåˆ—è¡¨
                    image_urls = [img.get("url", "") for img in images if img.get("url")]

                    if image_urls:
                        # è°ƒç”¨MCPä¸‹è½½å™¨
                        result_json = await download_files(
                            urls=image_urls,
                            save_path=str(save_dir) if save_dir else None,
                            max_concurrent=5
                        )
                        result = json.loads(result_json)
                        download_results = result.get("results", [])
                        success_count = result.get("success", 0)
                        failed_count = result.get("failed", 0)

                        print(f"    ä¸‹è½½å®Œæˆ: æˆåŠŸ {success_count} å¼ ï¼Œå¤±è´¥ {failed_count} å¼ ")
                        if save_dir:
                            print(f"    ä¿å­˜ä½ç½®: {save_dir.absolute()}")
                except Exception as e:
                    print(f"    ä¸‹è½½å¤±è´¥: {e}")

            # åˆ†æå›¾ç‰‡è´¨é‡
            valid_images = [img for img in images if _is_valid_content_image(img)]
            invalid_count = len(images) - len(valid_images)
            if invalid_count > 0:
                print(f"\n  [è´¨é‡åˆ†æ] å‘ç° {invalid_count} å¼ å¯èƒ½æ˜¯æ— å…³å›¾ç‰‡çš„é“¾æ¥")
        else:
            print("  [å›¾ç‰‡åˆ—è¡¨] æœªæ‰¾åˆ°å›¾ç‰‡")
            if status.get("status") != "ok":
                print(f"  åŸå› : {status.get('reason', 'æœªçŸ¥')}")

        return {
            "title": title,
            "url": url,
            "article_title": article_title,
            "content_length": content_length,
            "image_count": image_count,
            "images": images,
            "status": status,
            "success": True,
            "valid_content_images": len(
                [img for img in images if _is_valid_content_image(img)]
            ),
            "download_results": download_results,
        }

    except Exception as e:
        print(f"  âœ— é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()

        return {
            "title": title,
            "url": url,
            "error": str(e),
            "image_count": 0,
            "images": [],
            "success": False,
            "valid_content_images": 0,
        }


def _sanitize_filename(title: str) -> str:
    """å°†æ ‡é¢˜è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å

    Args:
        title: åŸå§‹æ ‡é¢˜

    Returns:
        å®‰å…¨çš„æ–‡ä»¶å
    """
    # ç§»é™¤æˆ–æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
    unsafe_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*', '\n', '\r', '\t']
    for char in unsafe_chars:
        title = title.replace(char, '_')

    # ç§»é™¤é¦–å°¾ç©ºæ ¼
    title = title.strip()

    # é™åˆ¶é•¿åº¦
    if len(title) > 100:
        title = title[:100]

    return title if title else "untitled"


def _is_valid_content_image(img: Dict) -> bool:
    """åˆ¤æ–­å›¾ç‰‡æ˜¯å¦å¯èƒ½æ˜¯æ­£æ–‡å›¾ç‰‡ï¼ˆè€Œéå¹¿å‘Šã€å›¾æ ‡ç­‰ï¼‰

    Args:
        img: å›¾ç‰‡ä¿¡æ¯å­—å…¸

    Returns:
        æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ­£æ–‡å›¾ç‰‡
    """
    url = img.get("url", "").lower()
    width = img.get("width", 0)
    height = img.get("height", 0)

    # æ£€æŸ¥å°ºå¯¸
    if width and height:
        if width < 150 or height < 150:
            return False

    # æ£€æŸ¥URLå…³é”®è¯
    unwanted_keywords = ["icon", "logo", "avatar", "ad", "banner", "share", "qr-code"]
    if any(kw in url for kw in unwanted_keywords):
        return False

    return True


async def run_test_suite(
    test_urls: List[Tuple[str, str]],
    save_result: bool = True,
    output_file: Path = Path("./test_data/image_extraction_test_results.json"),
    max_display_length: int = 70,
    download_images: bool = False,
    image_save_dir: Optional[Path] = None
):
    """è¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶

    Args:
        test_urls: æµ‹è¯•URLåˆ—è¡¨
        save_result: æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        max_display_length: æ§åˆ¶å°æ˜¾ç¤ºURLæœ€å¤§é•¿åº¦
        download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
        image_save_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
    """
    print("\n" + "ğŸš€" * 30)
    print(f"å›¾ç‰‡æå–åŠŸèƒ½æµ‹è¯• - å…± {len(test_urls)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    print("ğŸš€" * 30)

    all_results = []

    for idx, (url, title) in enumerate(test_urls, 1):
        print(f"\n\n[{idx}/{len(test_urls)}]")
        result = await test_single_url(
            url, title, max_display_length, download_images, image_save_dir
        )
        all_results.append(result)

    # æ‰“å°æµ‹è¯•æ±‡æ€»
    print_test_summary(all_results)

    # ä¿å­˜ç»“æœ
    if save_result:
        save_results(all_results, output_file)

    return all_results


def print_test_summary(results: List[Dict]):
    """æ‰“å°æµ‹è¯•æ±‡æ€»æŠ¥å‘Š

    Args:
        results: æµ‹è¯•ç»“æœåˆ—è¡¨
    """
    print("\n\n" + "=" * 60)
    print("æµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
    print("=" * 60)

    total_pages = len(results)
    successful_pages = sum(1 for r in results if r.get("success"))
    total_images = sum(r.get("image_count", 0) for r in results)
    total_valid_images = sum(r.get("valid_content_images", 0) for r in results)

    print(f"\n[ç»Ÿè®¡ä¿¡æ¯]")
    print(f"  æµ‹è¯•é¡µé¢æ•°: {total_pages}")
    print(
        f"  æˆåŠŸè·å–: {successful_pages} ({successful_pages * 100 // total_pages if total_pages else 0}%)"
    )
    print(f"  å¤±è´¥æ•°é‡: {total_pages - successful_pages}")
    print(f"  æ€»å›¾ç‰‡æ•°: {total_images}")
    print(
        f"  æœ‰æ•ˆæ­£æ–‡å›¾ç‰‡: {total_valid_images} ({total_valid_images * 100 // total_images if total_images else 0}%)"
    )

    # æŒ‰æˆåŠŸç‡åˆ†ç±»
    print(f"\n[è¯¦ç»†ç»“æœ]")
    for r in results:
        if r.get("success"):
            status = "âœ…"
            img_info = f"æ‰¾åˆ° {r['image_count']} å¼ å›¾ç‰‡"
            if r.get("valid_content_images", 0) != r["image_count"]:
                img_info += (
                    f" (å…¶ä¸­ {r.get('valid_content_images', 0)} å¼ å¯èƒ½æ˜¯æ­£æ–‡å›¾ç‰‡)"
                )
            content_info = f"ï¼Œæ­£æ–‡ {r['content_length']} å­—ç¬¦"
        else:
            status = "âŒ"
            img_info = f"é”™è¯¯: {r.get('error', 'æœªçŸ¥é”™è¯¯')}"
            content_info = ""

        print(f"  {status} {r['title']}")
        print(f"      {img_info}{content_info}")

    # è´¨é‡åˆ†æ
    if total_images > 0:
        invalid_ratio = (total_images - total_valid_images) / total_images * 100
        print(f"\n[è´¨é‡è¯„ä¼°]")
        if invalid_ratio < 10:
            print("  ğŸŒŸ ä¼˜ç§€ï¼šå‡ ä¹éƒ½æ˜¯æ­£æ–‡å›¾ç‰‡ï¼Œè¿‡æ»¤æ•ˆæœæä½³")
        elif invalid_ratio < 25:
            print("  ğŸ‘ è‰¯å¥½ï¼šå¤§éƒ¨åˆ†æ˜¯æ­£æ–‡å›¾ç‰‡ï¼Œæœ‰å°‘é‡è¯¯æå–")
        elif invalid_ratio < 50:
            print("  âš ï¸  ä¸€èˆ¬ï¼šå­˜åœ¨è¾ƒå¤šæ— å…³å›¾ç‰‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            print("  âŒ è¾ƒå·®ï¼šæå–äº†å¤§é‡æ— å…³å›¾ç‰‡ï¼Œå»ºè®®æ£€æŸ¥è¿‡æ»¤è§„åˆ™")


def save_results(results: List[Dict], output_file: Path):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶

    Args:
        results: æµ‹è¯•ç»“æœåˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\n[æ–‡ä»¶è¾“å‡º] æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file.absolute()}")


async def cleanup():
    """æ¸…ç†èµ„æº"""
    try:
        from mcp_server.web_browser.core.browser_pool import get_browser_pool

        browser_pool = get_browser_pool()
        await browser_pool.close()
        print("\n[æ¸…ç†] æµè§ˆå™¨èµ„æºå·²é‡Šæ”¾")
    except Exception as e:
        print(f"\n[è­¦å‘Š] æ¸…ç†æµè§ˆå™¨èµ„æºæ—¶å‡ºé”™: {e}")


# ==================== å…¥å£ç‚¹ ====================

if __name__ == "__main__":
    # ========== é…ç½®å‚æ•° ==========
    # åœ¨æ­¤å¤„ä¿®æ”¹æµ‹è¯•å‚æ•°ï¼Œæ— éœ€ä¿®æ”¹ä¸Šæ–¹ä»£ç 

    # æµ‹è¯•URLåˆ—è¡¨ - æ ¹æ®éœ€è¦ä¿®æ”¹æ­¤å¤„
    test_urls = [
        # ç§‘æŠ€æ–°é—»
        ("https://www.sohu.com/a/981633569_120244154", "æ–‡æ±‡æŠ¥-AGIä¸Šæµ·æ–¹æ¡ˆ"),
        ("https://news.qq.com/rain/a/20260128A06DCS00", "è…¾è®¯-å‘¨ä¼¯æ–‡ç‰¹é‚€æŠ¥å‘Š"),
        ("https://news.qq.com/rain/a/20260127A02ITV00", "è‡³é¡¶ç§‘æŠ€-ChartVerseå›¾è¡¨ç†è§£"),
        # ä½“è‚²æ–°é—»
        ("https://www.sohu.com/a/971359832_122219432", "æœç‹-æ¨ŠæŒ¯ä¸œé™ˆæ¢¦é€€å‡ºä¸–æ’"),
        ("https://news.qq.com/rain/a/20260130A03DFT00", "è…¾è®¯-NBAçƒ­ç«äº¤æ˜“ä¼ é—»"),
        # è´¢ç»æ–°é—»
        ("https://www.sohu.com/a/971226352_121106854", "æœç‹-è´µé‡‘å±å¸‚åœºæš´æ¶¨"),
    ]

    # æ˜¯å¦ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶
    save_result = True

    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = Path("./test_data/image_extraction_test_results.json")

    # æ§åˆ¶å°æ˜¾ç¤ºçš„URLæœ€å¤§é•¿åº¦
    max_display_length = 70

    # æ˜¯å¦ä¸‹è½½æå–åˆ°çš„å›¾ç‰‡
    download_images = True

    # å›¾ç‰‡ä¿å­˜ç›®å½•ï¼ˆä¸ºNoneæ—¶ä½¿ç”¨ä¸‹è½½å™¨é»˜è®¤ç›®å½•ï¼‰
    image_save_dir = Path("./test_data/downloaded_images")

    # ==================== æ‰§è¡Œæµ‹è¯• ====================
    asyncio.run(
        run_test_suite(
            test_urls,
            save_result=save_result,
            output_file=output_file,
            max_display_length=max_display_length,
            download_images=download_images,
            image_save_dir=image_save_dir,
        )
    )
