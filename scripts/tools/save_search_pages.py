"""
ä¿å­˜æœç´¢å¼•æ“é¡µé¢demo

ç”¨äºä¿å­˜å„ä¸ªæœç´¢å¼•æ“çš„å®é™…é¡µé¢ï¼Œæ–¹ä¾¿åˆ†æé¡µé¢ç»“æ„å¹¶è°ƒæ•´è§£æé€»è¾‘
"""

import sys
import io
import asyncio
from pathlib import Path

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from mcp_server.baidu_search.browser_pool import get_browser_pool


# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR = Path("search_engine_demos")
OUTPUT_DIR.mkdir(exist_ok=True)


async def save_search_page(engine_name: str, url: str, search_type: str):
    """ä¿å­˜æœç´¢å¼•æ“é¡µé¢"""
    print(f"\n{'='*60}")
    print(f"ğŸ“„ ä¿å­˜ {engine_name} {search_type} æœç´¢é¡µé¢")
    print(f"   URL: {url}")
    print(f"{'='*60}")

    # åˆå§‹åŒ–æµè§ˆå™¨æ± 
    browser_pool = get_browser_pool(
        max_concurrent=1,
        proxy={"server": "localhost:7897"}
    )

    try:
        async with browser_pool.get_page() as page:
            # è®¿é—®æœç´¢é¡µé¢
            print(f"ğŸŒ æ­£åœ¨è®¿é—®...")
            await page.goto(url, timeout=30000)

            # ç­‰å¾…é¡µé¢åŠ è½½
            await asyncio.sleep(3)

            # è·å–é¡µé¢æ ‡é¢˜
            title = await page.title()
            print(f"ğŸ“‹ é¡µé¢æ ‡é¢˜: {title}")

            # è·å–é¡µé¢HTML
            html = await page.content()

            # ä¿å­˜åˆ°æ–‡ä»¶
            filename = OUTPUT_DIR / f"{engine_name}_{search_type}.html"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(html)

            print(f"âœ… å·²ä¿å­˜åˆ°: {filename}")
            print(f"   æ–‡ä»¶å¤§å°: {len(html)} å­—ç¬¦")

            # åŒæ—¶ä¿å­˜é¡µé¢æˆªå›¾
            screenshot_path = OUTPUT_DIR / f"{engine_name}_{search_type}.png"
            await page.screenshot(path=str(screenshot_path), full_page=True)
            print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {screenshot_path}")

            return True

    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        await browser_pool.close()


async def save_all_search_pages():
    """ä¿å­˜æ‰€æœ‰æœç´¢å¼•æ“çš„é¡µé¢demo"""
    print("\n" + "="*60)
    print("ğŸ”¥ æœç´¢å¼•æ“é¡µé¢demoä¿å­˜å·¥å…·")
    print("="*60)
    print(f"\nè¾“å‡ºç›®å½•: {OUTPUT_DIR.absolute()}")

    # æœç´¢å¼•æ“URLé…ç½®ï¼ˆåªä¿ç•™æ–°é—»æœç´¢ï¼‰
    search_engines = {
        "ç™¾åº¦": {
            "news": "https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&ie=utf-8&word=%E4%BD%A0%E5%A5%BD",
        },
        # "æœç‹—": {
        #     "news": "https://www.sogou.com/sogou?ie=utf8&p=40230447&interation=1728053249&interV=&pid=sogou-wsse-8f646834ef1adefa&query=%E4%BD%A0%E5%A5%BD",
        # },
        # "å¿…åº”": {
        #     "news": "https://www.bing.com/news/search?q=%E4%BD%A0%E5%A5%BD",
        # },
        # "è°·æ­Œ": {
        #     "news": "https://www.google.com/search?q=%E4%BD%A0%E5%A5%BD&tbm=nws",
        # },
    }

    success_count = 0
    total_count = 0

    # éå†æ‰€æœ‰æœç´¢å¼•æ“
    for engine_name, urls in search_engines.items():
        for search_type, url in urls.items():
            total_count += 1
            success = await save_search_page(engine_name, url, search_type)
            if success:
                success_count += 1

            # ç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(3)

    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š ä¿å­˜æ€»ç»“")
    print("="*60)
    print(f"æˆåŠŸ: {success_count}/{total_count}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR.absolute()}")

    if success_count == total_count:
        print("\nâœ… æ‰€æœ‰é¡µé¢ä¿å­˜æˆåŠŸï¼")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥ï¼š")
        print("1. æŸ¥çœ‹ä¿å­˜çš„HTMLæ–‡ä»¶ï¼Œåˆ†æé¡µé¢ç»“æ„")
        print("2. æŸ¥çœ‹æˆªå›¾ï¼Œäº†è§£é¡µé¢å¸ƒå±€")
        print("3. å‘Šè¯‰æˆ‘é¡µé¢ç»“æ„ï¼Œæˆ‘å°†è°ƒæ•´è§£æé€»è¾‘")
    else:
        print("\nâš ï¸ éƒ¨åˆ†é¡µé¢ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")


async def save_custom_search_page(engine_name: str, url: str, search_type: str = "web"):
    """ä¿å­˜è‡ªå®šä¹‰æœç´¢é¡µé¢"""
    await save_search_page(engine_name, url, search_type)


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        # è‡ªå®šä¹‰URLæ¨¡å¼
        if len(sys.argv) >= 3:
            engine = sys.argv[1]
            url = sys.argv[2]
            search_type = sys.argv[3] if len(sys.argv) > 3 else "web"
            asyncio.run(save_custom_search_page(engine, url, search_type))
        else:
            print("ç”¨æ³•: python save_search_pages.py <å¼•æ“å> <URL> [æœç´¢ç±»å‹]")
            print("ç¤ºä¾‹: python save_search_pages.py ç™¾åº¦ 'https://www.baidu.com/s?wd=test' web")
    else:
        # é»˜è®¤ï¼šä¿å­˜æ‰€æœ‰æœç´¢å¼•æ“
        asyncio.run(save_all_search_pages())
