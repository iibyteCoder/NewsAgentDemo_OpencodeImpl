"""
æµ‹è¯•æœç´¢å¼•æ“é€Ÿåº¦å¹¶æ’åº
"""

import sys
import io
import asyncio
import json
from typing import List, Dict
import time

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from mcp_server.baidu_search.multi_engine import MultiSearchEngine, get_multi_engine
from mcp_server.baidu_search.browser_pool import get_browser_pool
from mcp_server.baidu_search.main import rate_limiter


async def test_engine_speed(
    engine_name: str,
    query: str = "ä½ å¥½",
    search_type: str = "news",
    repeat: int = 3
) -> Dict:
    """æµ‹è¯•å•ä¸ªæœç´¢å¼•æ“çš„é€Ÿåº¦

    Args:
        engine_name: æœç´¢å¼•æ“åç§°
        query: æœç´¢å…³é”®è¯
        search_type: æœç´¢ç±»å‹ (web/news)
        repeat: é‡å¤æµ‹è¯•æ¬¡æ•°

    Returns:
        åŒ…å«é€Ÿåº¦ç»Ÿè®¡çš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"æµ‹è¯• {engine_name} æœç´¢å¼•æ“é€Ÿåº¦")
    print(f"å…³é”®è¯: {query}")
    print(f"é‡å¤æ¬¡æ•°: {repeat}")
    print('='*60)

    # åˆå§‹åŒ–
    browser_pool = get_browser_pool(
        max_concurrent=1,
        proxy={"server": "localhost:7897"}
    )
    multi_engine = get_multi_engine(browser_pool, rate_limiter)

    times = []
    success_count = 0
    error_count = 0

    for i in range(repeat):
        try:
            print(f"\nç¬¬ {i+1}/{repeat} æ¬¡æµ‹è¯•...")

            start_time = time.time()

            # æ‰§è¡Œæœç´¢
            result_json = await multi_engine.search(
                query=query,
                engine=engine_name,
                num_results=10,
                search_type=search_type
            )

            end_time = time.time()
            elapsed = end_time - start_time

            # è§£æç»“æœ
            result = json.loads(result_json)

            if result.get('error'):
                print(f"   âŒ é”™è¯¯: {result['error']}")
                error_count += 1
            else:
                total = result.get('total', 0)
                print(f"   âœ… æˆåŠŸ! è€—æ—¶: {elapsed:.2f}ç§’, ç»“æœæ•°: {total}")
                times.append(elapsed)
                success_count += 1

        except Exception as e:
            print(f"   âŒ å¼‚å¸¸: {e}")
            error_count += 1

        # æ¯æ¬¡æµ‹è¯•ä¹‹é—´ç­‰å¾…ä¸€ä¸‹
        if i < repeat - 1:
            await asyncio.sleep(2)

    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    stats = {
        "engine": engine_name,
        "success_count": success_count,
        "error_count": error_count,
        "times": times,
    }

    if times:
        stats["avg_time"] = sum(times) / len(times)
        stats["min_time"] = min(times)
        stats["max_time"] = max(times)
        stats["success_rate"] = success_count / repeat
    else:
        stats["avg_time"] = float('inf')
        stats["min_time"] = float('inf')
        stats["max_time"] = float('inf')
        stats["success_rate"] = 0.0

    # æ‰“å°ç»Ÿè®¡
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"   æˆåŠŸæ¬¡æ•°: {success_count}/{repeat}")
    print(f"   å¤±è´¥æ¬¡æ•°: {error_count}")
    if times:
        print(f"   å¹³å‡è€—æ—¶: {stats['avg_time']:.2f}ç§’")
        print(f"   æœ€å¿«: {stats['min_time']:.2f}ç§’")
        print(f"   æœ€æ…¢: {stats['max_time']:.2f}ç§’")
    print(f"   æˆåŠŸç‡: {stats['success_rate']*100:.1f}%")

    await browser_pool.close()

    return stats


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸš€ æœç´¢å¼•æ“é€Ÿåº¦æµ‹è¯•")
    print("="*60)

    test_query = "äººå·¥æ™ºèƒ½"
    search_type = "news"
    repeat = 3  # æ¯ä¸ªå¼•æ“æµ‹è¯•3æ¬¡

    # è¦æµ‹è¯•çš„å¼•æ“åˆ—è¡¨
    engines = [
        "baidu",
        "bing",
        "sogou",
        "google",
        "360",
    ]

    # æµ‹è¯•æ‰€æœ‰å¼•æ“
    all_stats = []
    for engine in engines:
        try:
            stats = await test_engine_speed(engine, test_query, search_type, repeat)
            all_stats.append(stats)
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• {engine} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

    # æ’åºå¼•æ“ï¼ˆæŒ‰å¹³å‡è€—æ—¶ï¼‰
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆæ’åï¼ˆæŒ‰å¹³å‡é€Ÿåº¦æ’åºï¼‰")
    print("="*60)

    # åªä¿ç•™æˆåŠŸçš„æµ‹è¯•
    successful_stats = [s for s in all_stats if s['times']]

    if not successful_stats:
        print("\nâŒ æ‰€æœ‰å¼•æ“æµ‹è¯•éƒ½å¤±è´¥äº†ï¼")
        return

    # æŒ‰å¹³å‡æ—¶é—´æ’åºï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼‰
    successful_stats.sort(key=lambda x: x['avg_time'])

    print(f"\n{'æ’å':<6}{'å¼•æ“':<12}{'å¹³å‡è€—æ—¶':<12}{'æœ€å¿«':<10}{'æœ€æ…¢':<10}{'æˆåŠŸç‡':<10}")
    print("-" * 60)

    for rank, stats in enumerate(successful_stats, 1):
        print(
            f"{rank:<6}"
            f"{stats['engine']:<12}"
            f"{stats['avg_time']:.2f}ç§’     "
            f"{stats['min_time']:.2f}ç§’  "
            f"{stats['max_time']:.2f}ç§’  "
            f"{stats['success_rate']*100:.0f}%"
        )

    # ç”Ÿæˆæ¨èçš„å¼•æ“é¡ºåºé…ç½®
    print("\n" + "="*60)
    print("ğŸ”§ æ¨èçš„å¼•æ“ä¼˜å…ˆçº§é…ç½®")
    print("="*60)

    recommended_order = [s['engine'] for s in successful_stats]

    print(f"\nå¼•æ“é¡ºåº = {recommended_order}")
    print(f"\nè¯´æ˜: é€Ÿåº¦å¿«çš„å¼•æ“ä¼˜å…ˆä½¿ç”¨ï¼Œå¯ä»¥æé«˜æ•´ä½“å“åº”é€Ÿåº¦ã€‚")

    # ä¿å­˜åˆ°æ–‡ä»¶
    with open('engine_speed_ranking.json', 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'test_query': test_query,
            'search_type': search_type,
            'repeat': repeat,
            'ranking': successful_stats,
            'recommended_order': recommended_order
        }, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: engine_speed_ranking.json")


if __name__ == "__main__":
    asyncio.run(main())
