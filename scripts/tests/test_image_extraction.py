"""
æµ‹è¯•æ–‡ç« å†…å®¹è·å–ä¸å›¾ç‰‡é“¾æ¥æå–åŠŸèƒ½

æµ‹è¯• fetch_article_content å‡½æ•°ï¼š
- è®¿é—®æ–°é—»ç½‘é¡µ
- æå–æ–‡ç« å†…å®¹
- æå–å›¾ç‰‡é“¾æ¥

ç”¨æ³•ï¼š
1. åœ¨ VS Code ä¸­å³é”® -> "åœ¨ç»ˆç«¯ä¸­è¿è¡Œ Python æ–‡ä»¶" æˆ–æŒ‰ F5 ç›´æ¥è°ƒè¯•
2. æˆ–è€…ç‚¹å‡»å³ä¸Šè§’çš„è¿è¡ŒæŒ‰é’®

æ¥æºï¼š2025å¹´å›½å†…å›½é™…åå¤§ä½“è‚²æ–°é—»è¯„é€‰äº‹ä»¶
"""

import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from mcp_server.web_browser.tools.search_tools import fetch_article_content


# ==================== æµ‹è¯•å‡½æ•° ====================

async def test_fetch_article_with_images(urls, save_result=True):
    """æµ‹è¯•ä»æ–°é—»URLè·å–æ–‡ç« å†…å®¹å’Œå›¾ç‰‡é“¾æ¥

    Args:
        urls: URLåˆ—è¡¨ï¼Œæ ¼å¼ä¸º [(url, title), ...]
        save_result: æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    """
    print("\n" + "=" * 60)
    print("æµ‹è¯• fetch_article_content - è·å–æ–‡ç« å†…å®¹å’Œå›¾ç‰‡é“¾æ¥")
    print("=" * 60)
    print(f"\nå°†æµ‹è¯• {len(urls)} ä¸ªæ–°é—»URL")

    all_results = []

    for idx, (url, title) in enumerate(urls, 1):
        print(f"\n[{idx}/{len(urls)}] {title}")
        print(f"URL: {url}")
        print("-" * 60)

        try:
            # è°ƒç”¨ fetch_article_content è·å–æ–‡ç« å†…å®¹å’Œå›¾ç‰‡é“¾æ¥
            print("  æ­£åœ¨è·å–æ–‡ç« å†…å®¹...")
            result_json = await fetch_article_content(url, include_images=True)

            # è§£æç»“æœ
            result = json.loads(result_json)

            # æå–ä¿¡æ¯
            article_title = result.get('title', 'æ— æ ‡é¢˜')
            content_length = result.get('content_length', 0)
            images = result.get('images', [])
            image_count = result.get('image_count', len(images))
            status = result.get('status', {})

            print(f"  æ ‡é¢˜: {article_title}")
            print(f"  æ­£æ–‡é•¿åº¦: {content_length} å­—ç¬¦")
            print(f"  é¡µé¢çŠ¶æ€: {status.get('status', 'unknown')}")
            print(f"  å›¾ç‰‡æ•°é‡: {image_count}")

            if images:
                print(f"\n  å›¾ç‰‡é“¾æ¥åˆ—è¡¨:")
                for i, img in enumerate(images, 1):
                    img_url = img.get('url', 'N/A')
                    alt = img.get('alt', '')
                    # æ˜¾ç¤ºURLï¼Œå¦‚æœå¤ªé•¿å°±æˆªæ–­
                    display_url = img_url if len(img_url) <= 70 else img_url[:67] + "..."
                    alt_text = f" - {alt}" if alt else ""
                    print(f"    {i}. {display_url}{alt_text}")
            else:
                print("  æœªæ‰¾åˆ°å›¾ç‰‡")

            all_results.append({
                'title': title,
                'url': url,
                'article_title': article_title,
                'content_length': content_length,
                'image_count': image_count,
                'images': images,
                'status': status,
                'success': True
            })

        except Exception as e:
            print(f"  âœ— é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

            all_results.append({
                'title': title,
                'url': url,
                'error': str(e),
                'image_count': 0,
                'images': [],
                'success': False
            })

    # æ±‡æ€»ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ±‡æ€»")
    print("=" * 60)

    total_pages = len(urls)
    total_images = sum(r.get('image_count', 0) for r in all_results)
    successful_pages = sum(1 for r in all_results if r.get('success'))

    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æµ‹è¯•é¡µé¢æ•°: {total_pages}")
    print(f"  æˆåŠŸè·å–: {successful_pages}")
    print(f"  æ€»å…±æ‰¾åˆ°å›¾ç‰‡é“¾æ¥: {total_images}")

    print(f"\nè¯¦ç»†ç»“æœ:")
    for r in all_results:
        if r.get('success'):
            status = "âœ“"
            detail = f"æ‰¾åˆ° {r['image_count']} ä¸ªå›¾ç‰‡é“¾æ¥ï¼Œæ­£æ–‡ {r['content_length']} å­—ç¬¦"
        else:
            status = "âœ—"
            detail = f"é”™è¯¯ - {r.get('error', 'æœªçŸ¥é”™è¯¯')}"

        print(f"  {status} {r['title']}")
        print(f"      {detail}")

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if save_result:
        save_results_to_file(all_results)

    return all_results


def save_results_to_file(results):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
    output_file = Path("./test_data/article_fetch_result.json")
    output_file.parent.mkdir(parents=True, exist_ok=True)

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file.absolute()}")


async def main(test_urls):
    """ä¸»è¿è¡Œå‡½æ•°"""
    print("\n" + "ğŸš€" * 30)
    print("å¼€å§‹æµ‹è¯• fetch_article_content åŠŸèƒ½")
    print("ğŸš€" * 30)

    try:
        # è¿è¡Œæµ‹è¯•
        await test_fetch_article_with_images(test_urls, save_result=True)

        print("\n" + "âœ…" * 30)
        print("æµ‹è¯•å®Œæˆï¼")
        print("âœ…" * 30)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # æ¸…ç†èµ„æº
        from mcp_server.web_browser.core.browser_pool import get_browser_pool
        browser_pool = get_browser_pool()
        await browser_pool.close()


if __name__ == "__main__":
    # ==================== é…ç½®é¡¹ ====================
    # çœŸå®æ–°é—»URLï¼ˆæ¥è‡ªä½“è‚²æ–°é—»æ±‡æ€»æŠ¥å‘Šï¼‰
    test_urls = [
        ("https://www.sohu.com/a/971359832_122219432", "æœç‹-æ¨ŠæŒ¯ä¸œé™ˆæ¢¦é€€å‡ºä¸–æ’"),
        # ("https://news.qq.com/rain/a/20260109A03DFT00", "è…¾è®¯-èµµå¿ƒç«¥æ–¯è¯ºå…‹å¤ºå† "),
        # ("https://news.qq.com/rain/a/20250626A09B5B00", "è…¾è®¯-æ¨ç€šæ£®NBAé€‰ç§€"),
        # ("https://www.sohu.com/a/969024685_122014422", "æœç‹-2025å›½å†…åå¤§ä½“è‚²æ–°é—»"),
        # ("https://www.sohu.com/a/971226352_121106854", "æœç‹-2025å›½é™…åå¤§ä½“è‚²æ–°é—»"),
        # ("https://www.sport.gov.cn/n20001280/n20067662/n20067613/c29329796/content.html", "å›½å®¶ä½“è‚²æ€»å±€-åå¤§ä½“è‚²æ–°é—»"),
    ]

    asyncio.run(main(test_urls))
