"""
MCP Server å¹¶å‘æ€§èƒ½æµ‹è¯•

æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import sys
import io
import time
import asyncio
from typing import List, Dict
import json

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from mcp_server.baidu_search.main import (
    baidu_search,
    baidu_news_search,
    baidu_hot_search,
)
from mcp_server.baidu_search.browser_pool import get_browser_pool


class ConcurrentTester:
    """å¹¶å‘æµ‹è¯•å™¨"""

    def __init__(self):
        self.browser_pool = get_browser_pool()
        self.results = []

    async def test_single_search(self, query: str, index: int) -> Dict:
        """æ‰§è¡Œå•ä¸ªæœç´¢æµ‹è¯•"""
        start_time = time.time()
        try:
            result = await baidu_news_search(query=query, num_results=10)
            elapsed = time.time() - start_time

            data = json.loads(result)
            success = data.get("total", 0) > 0

            return {
                "index": index,
                "query": query,
                "success": success,
                "elapsed": elapsed,
                "result_count": data.get("total", 0),
            }
        except Exception as e:
            elapsed = time.time() - start_time
            return {
                "index": index,
                "query": query,
                "success": False,
                "elapsed": elapsed,
                "error": str(e),
            }

    async def test_concurrent_searches(
        self,
        queries: List[str],
        concurrency: int,
        batch_name: str,
    ) -> Dict:
        """æµ‹è¯•å¹¶å‘æœç´¢

        Args:
            queries: æœç´¢å…³é”®è¯åˆ—è¡¨
            concurrency: å¹¶å‘æ•°é‡
            batch_name: æ‰¹æ¬¡åç§°

        Returns:
            æµ‹è¯•ç»“æœç»Ÿè®¡
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æµ‹è¯•æ‰¹æ¬¡: {batch_name}")
        print(f"   å¹¶å‘æ•°: {concurrency}")
        print(f"   è¯·æ±‚æ•°: {len(queries)}")
        print(f"{'='*60}")

        start_time = time.time()

        # ä½¿ç”¨ Semaphore æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(concurrency)

        async def bounded_search(query: str, index: int):
            async with semaphore:
                return await self.test_single_search(query, index)

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æœç´¢
        tasks = [bounded_search(query, i) for i, query in enumerate(queries)]
        results = await asyncio.gather(*tasks)

        total_time = time.time() - start_time

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r["success"])
        failed_count = len(results) - success_count
        elapsed_times = [r["elapsed"] for r in results if r["success"]]

        stats = {
            "batch_name": batch_name,
            "concurrency": concurrency,
            "total_requests": len(queries),
            "success_count": success_count,
            "failed_count": failed_count,
            "total_time": total_time,
            "avg_time": sum(elapsed_times) / len(elapsed_times) if elapsed_times else 0,
            "min_time": min(elapsed_times) if elapsed_times else 0,
            "max_time": max(elapsed_times) if elapsed_times else 0,
            "throughput": len(queries) / total_time if total_time > 0 else 0,
            "results": results,
        }

        # æ‰“å°ç»Ÿè®¡
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"   æˆåŠŸ: {success_count} | å¤±è´¥: {failed_count}")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {stats['avg_time']:.2f}ç§’")
        print(f"   æœ€å¿«: {stats['min_time']:.2f}ç§’ | æœ€æ…¢: {stats['max_time']:.2f}ç§’")
        print(f"   ååé‡: {stats['throughput']:.2f} è¯·æ±‚/ç§’")

        return stats

    async def test_progressive_concurrency(self):
        """æ¸è¿›å¼å¹¶å‘æµ‹è¯•

        ä»ä½å¹¶å‘åˆ°é«˜å¹¶å‘ï¼Œé€æ­¥æµ‹è¯•
        """
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹æ¸è¿›å¼å¹¶å‘æµ‹è¯•")
        print("="*60)

        # æµ‹è¯•æ•°æ®ï¼ˆä¸åŒå…³é”®è¯ï¼‰
        test_queries = [
            "äººå·¥æ™ºèƒ½",
            "æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ä¹ ",
            "Pythonç¼–ç¨‹",
            "æ•°æ®ç§‘å­¦",
            "åŒºå—é“¾æŠ€æœ¯",
            "äº‘è®¡ç®—",
            "ç½‘ç»œå®‰å…¨",
            "5GæŠ€æœ¯",
            "é‡å­è®¡ç®—",
        ]

        all_stats = []

        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [1, 2, 3, 5, 8, 10]

        for concurrency in concurrency_levels:
            # é€‰æ‹©å¯¹åº”æ•°é‡çš„æŸ¥è¯¢
            queries = (test_queries * ((concurrency // len(test_queries)) + 1))[:concurrency]

            stats = await self.test_concurrent_searches(
                queries=queries,
                concurrency=concurrency,
                batch_name=f"å¹¶å‘çº§åˆ« {concurrency}",
            )
            all_stats.append(stats)

            # ç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…è¿ç»­æµ‹è¯•å½±å“
            await asyncio.sleep(2)

        # æ‰“å°æ€»ç»“
        self._print_summary(all_stats)

        return all_stats

    def _print_summary(self, all_stats: List[Dict]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*60)
        print("ğŸ“Š å¹¶å‘æµ‹è¯•æ€»ç»“")
        print("="*60)
        print(f"\n{'å¹¶å‘æ•°':<8} {'è¯·æ±‚æ•°':<8} {'æˆåŠŸæ•°':<8} {'æ€»è€—æ—¶':<10} {'å¹³å‡å“åº”':<10} {'ååé‡':<12}")
        print("-"*60)

        for stats in all_stats:
            print(
                f"{stats['concurrency']:<8} "
                f"{stats['total_requests']:<8} "
                f"{stats['success_count']:<8} "
                f"{stats['total_time']:<10.2f} "
                f"{stats['avg_time']:<10.2f} "
                f"{stats['throughput']:<12.2f}"
            )

        # æ€§èƒ½åˆ†æ
        print("\nğŸ” æ€§èƒ½åˆ†æ:")

        # æ‰¾å‡ºæœ€ä½³å¹¶å‘æ•°
        best_throughput = max(all_stats, key=lambda x: x["throughput"])
        print(f"   â€¢ æœ€ä½³ååé‡: {best_throughput['throughput']:.2f} è¯·æ±‚/ç§’ (å¹¶å‘={best_throughput['concurrency']})")

        # æ‰¾å‡ºæœ€å¿«å¹³å‡å“åº”
        best_avg_time = min(all_stats, key=lambda x: x["avg_time"])
        print(f"   â€¢ æœ€å¿«å¹³å‡å“åº”: {best_avg_time['avg_time']:.2f}ç§’ (å¹¶å‘={best_avg_time['concurrency']})")

        # æˆåŠŸç‡
        total_success = sum(s["success_count"] for s in all_stats)
        total_requests = sum(s["total_requests"] for s in all_stats)
        success_rate = (total_success / total_requests * 100) if total_requests > 0 else 0
        print(f"   â€¢ æ•´ä½“æˆåŠŸç‡: {success_rate:.1f}% ({total_success}/{total_requests})")


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸ”¥ MCP Server å¹¶å‘æ€§èƒ½æµ‹è¯•")
    print("="*60)

    # æ‰“å°æµè§ˆå™¨æ± é…ç½®
    browser_pool = get_browser_pool()
    print(f"\nğŸ“‹ æµè§ˆå™¨æ± é…ç½®:")
    print(f"   â€¢ æœ€å¤§å¹¶å‘æ•°: {browser_pool.max_concurrent_browsers}")
    print(f"   â€¢ æœ€å¤§ä¸Šä¸‹æ–‡æ•°: {browser_pool.max_contexts_per_browser}")

    tester = ConcurrentTester()

    try:
        # è¿è¡Œæ¸è¿›å¼å¹¶å‘æµ‹è¯•
        await tester.test_progressive_concurrency()

        # æ‰“å°æµè§ˆå™¨æ± ç»Ÿè®¡
        stats = browser_pool.get_stats()
        print(f"\nğŸ“Š æµè§ˆå™¨æ± ç»Ÿè®¡:")
        print(f"   â€¢ æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"   â€¢ å½“å‰æ´»è·ƒ: {stats['active_requests']}")
        print(f"   â€¢ æµè§ˆå™¨çŠ¶æ€: {'è¿è¡Œä¸­' if stats['browser_alive'] else 'å·²å…³é—­'}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # æ¸…ç†èµ„æº
        print("\nğŸ§¹ æ¸…ç†èµ„æº...")
        await browser_pool.close()
        print("âœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
