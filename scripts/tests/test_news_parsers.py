"""
æµ‹è¯•å„æœç´¢å¼•æ“æ–°é—»è§£æå™¨ - é‡æ„ç‰ˆ
"""

import sys
import io
import asyncio

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ä½¿ç”¨æ–°çš„æ¨¡å—ç»“æ„
from mcp_server.baidu_search.config.settings import get_settings
from mcp_server.baidu_search.core import get_browser_pool, RateLimiter
from mcp_server.baidu_search.engines import EngineFactory
from mcp_server.baidu_search.utils.helpers import get_random_user_agent, search_result_to_dict


async def test_engine_news(engine_name: str, query: str):
    """æµ‹è¯•å•ä¸ªæœç´¢å¼•æ“çš„æ–°é—»è§£æ"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯• {engine_name} æ–°é—»æœç´¢")
    print(f"å…³é”®è¯: {query}")
    print('='*60)

    # åˆå§‹åŒ–
    settings = get_settings()
    browser_pool = get_browser_pool(settings)
    rate_limiter = RateLimiter(
        time_window=settings.rate_limit_time_window,
        max_domain_requests=settings.max_domain_requests_per_second,
        max_engine_requests=settings.max_engine_requests_per_second,
    )
    engine_factory = EngineFactory(enabled_engines=settings.enabled_engines)

    try:
        # è·å–å¼•æ“
        engine = engine_factory.get_engine(engine_name)
        if not engine:
            print(f"\nâŒ å¼•æ“ {engine_name} ä¸å¯ç”¨")
            return False

        # åº”ç”¨é€Ÿç‡é™åˆ¶
        search_url = engine.get_search_url(query, 10, "news")
        domain = engine.extract_domain(search_url)
        await rate_limiter.acquire(domain=domain, engine=engine_name)

        # æ‰§è¡Œæœç´¢
        user_agent = get_random_user_agent()
        async with browser_pool.get_page(user_agent=user_agent) as page:
            await page.goto(search_url, timeout=30000)

            # è§£æç»“æœ
            results = await engine.search(page, query, 10, "news")

            # è½¬æ¢ä¸ºå­—å…¸
            results_dict = [search_result_to_dict(r) for r in results]

            print(f"\nå¼•æ“: {engine.config.name}")
            print(f"æŸ¥è¯¢: {query}")
            print(f"ç»“æœæ•°é‡: {len(results_dict)}")

            # æ˜¾ç¤ºå‰5æ¡ç»“æœ
            print(f"\nå‰5æ¡ç»“æœ:")
            print(f"{'-'*60}")

            for i, item in enumerate(results_dict[:5], 1):
                print(f"\n{i}. {item.get('title', 'N/A')}")
                print(f"   æ¥æº: {item.get('source', 'N/A')}")
                print(f"   æ—¶é—´: {item.get('time', 'N/A')}")
                url = item.get('url', 'N/A')
                print(f"   é“¾æ¥: {url[:80] if url != 'N/A' else 'N/A'}...")
                if item.get('summary'):
                    print(f"   æ‘˜è¦: {item['summary'][:100]}...")

            return len(results_dict) > 0

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        await browser_pool.close()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸ”¥ æœç´¢å¼•æ“æ–°é—»è§£æå™¨æµ‹è¯•")
    print("="*60)

    test_query = "ä½ å¥½"

    engines = [
        # "baidu",
        # "bing",
        # "google",
        # "sogou",
        "360",
    ]

    success_count = 0
    total_count = len(engines)

    for engine in engines:
        success = await test_engine_news(engine, test_query)
        if success:
            success_count += 1

        # ç­‰å¾…ä¸€ä¸‹ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(3)

    # æ‰“å°æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"æˆåŠŸ: {success_count}/{total_count}")

    if success_count == total_count:
        print("\nâœ… æ‰€æœ‰å¼•æ“æµ‹è¯•é€šè¿‡ï¼")
    else:
        print(f"\nâš ï¸ {total_count - success_count} ä¸ªå¼•æ“æµ‹è¯•å¤±è´¥")


if __name__ == "__main__":
    asyncio.run(main())
