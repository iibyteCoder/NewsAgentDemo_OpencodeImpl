"""
æµ‹è¯•å¼‚æ­¥æ–°é—»å­˜å‚¨åŠŸèƒ½
"""

import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger
from mcp_server.news_storage.core.database import get_database, NewsItem


async def test_database():
    """æµ‹è¯•æ•°æ®åº“æ‰€æœ‰åŠŸèƒ½"""

    logger.info("ğŸš€ å¼€å§‹æµ‹è¯•å¼‚æ­¥æ•°æ®åº“åŠŸèƒ½")

    # 1. è·å–æ•°æ®åº“å®ä¾‹
    db = await get_database()
    logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")

    # 2. æµ‹è¯•ä¿å­˜å•æ¡æ–°é—»
    logger.info("\nğŸ“ æµ‹è¯•ä¿å­˜å•æ¡æ–°é—»...")
    news1 = NewsItem(
        title="AIæŠ€æœ¯é‡å¤§çªç ´ï¼GPT-5å³å°†å‘å¸ƒ",
        url="https://example.com/news/ai-breakthrough",
        summary="OpenAIå®£å¸ƒå°†åœ¨ä¸‹ä¸ªæœˆå‘å¸ƒGPT-5ï¼Œæ€§èƒ½æå‡æ˜¾è‘—",
        source="ç§‘æŠ€æ—¥æŠ¥",
        publish_time="2026-01-29",
        author="å¼ ä¸‰",
        event_name="2026å¹´AIæŠ€æœ¯çªç ´",
        content="è¿™æ˜¯å®Œæ•´çš„æ–°é—»å†…å®¹...",
        html_content="<p>è¿™æ˜¯HTMLå†…å®¹</p>",
        keywords=["AI", "GPT-5", "ç§‘æŠ€"],
        image_urls=["https://example.com/img1.jpg"],
        local_image_paths=["./data/images/img1.jpg"],
        tags=["ç§‘æŠ€", "äººå·¥æ™ºèƒ½"],
    )
    is_new = await db.save_news(news1)
    logger.info(f"{'âœ… æ–°å¢æˆåŠŸ' if is_new else 'ğŸ”„ æ›´æ–°æˆåŠŸ'}: {news1.title}")

    # 3. æµ‹è¯•ä¿å­˜ç¬¬äºŒæ¡æ–°é—»
    logger.info("\nğŸ“ æµ‹è¯•ä¿å­˜ç¬¬äºŒæ¡æ–°é—»...")
    news2 = NewsItem(
        title="æ¬§å† è”èµ›ï¼šçš‡é©¬é€†è½¬å·´é»åœ£æ—¥è€³æ›¼",
        url="https://example.com/sports/ufcl",
        summary="çš‡é©¬åœ¨ä¸»åœº3-2é€†è½¬å·´é»åœ£æ—¥è€³æ›¼ï¼Œæ™‹çº§æ¬§å† å…«å¼º",
        source="ä½“è‚²å‘¨åˆŠ",
        publish_time="2026-01-28",
        author="æå››",
        event_name="æ¬§å† è”èµ›",
        keywords=["æ¬§å† ", "çš‡é©¬", "è¶³çƒ"],
        tags=["ä½“è‚²", "è¶³çƒ"],
    )
    await db.save_news(news2)
    logger.info(f"âœ… ä¿å­˜æˆåŠŸ: {news2.title}")

    # 4. æµ‹è¯•æ‰¹é‡ä¿å­˜
    logger.info("\nğŸ“¦ æµ‹è¯•æ‰¹é‡ä¿å­˜...")
    news_batch = [
        NewsItem(
            title=f"æµ‹è¯•æ–°é—»{i}",
            url=f"https://example.com/test/{i}",
            source="æµ‹è¯•æº",
            keywords=["æµ‹è¯•"],
        )
        for i in range(3, 6)
    ]
    stats = await db.save_news_batch(news_batch)
    logger.info(f"âœ… æ‰¹é‡ä¿å­˜å®Œæˆ: {stats}")

    # 5. æµ‹è¯•æ ¹æ®URLè·å–æ–°é—»
    logger.info("\nğŸ” æµ‹è¯•æ ¹æ®URLè·å–æ–°é—»...")
    found_news = await db.get_news_by_url(news1.url)
    if found_news:
        logger.info(f"âœ… æ‰¾åˆ°æ–°é—»: {found_news.title}")
        logger.info(f"   å…³é”®è¯: {found_news.keywords}")
        logger.info(f"   æœ¬åœ°å›¾ç‰‡: {found_news.local_image_paths}")
    else:
        logger.error("âŒ æœªæ‰¾åˆ°æ–°é—»")

    # 6. æµ‹è¯•æœç´¢åŠŸèƒ½
    logger.info("\nğŸ” æµ‹è¯•æœç´¢åŠŸèƒ½...")

    # æœç´¢AIç›¸å…³
    from mcp_server.news_storage.core.models import SearchFilter

    filter1 = SearchFilter(search_terms=["AI", "GPT"], limit=10)
    results1 = await db.search_news(filter1)
    logger.info(f"âœ… æœç´¢ 'AI GPT': æ‰¾åˆ° {len(results1)} æ¡ç»“æœ")
    for news in results1:
        logger.info(f"   - {news.title}")

    # æŒ‰æ¥æºç­›é€‰
    filter2 = SearchFilter(source="ç§‘æŠ€æ—¥æŠ¥", limit=10)
    results2 = await db.search_news(filter2)
    logger.info(f"âœ… æŒ‰æ¥æº 'ç§‘æŠ€æ—¥æŠ¥' ç­›é€‰: æ‰¾åˆ° {len(results2)} æ¡ç»“æœ")

    # æŒ‰äº‹ä»¶åç§°ç­›é€‰
    filter3 = SearchFilter(event_name="æ¬§å† è”èµ›", limit=10)
    results3 = await db.search_news(filter3)
    logger.info(f"âœ… æŒ‰äº‹ä»¶ 'æ¬§å† è”èµ›' ç­›é€‰: æ‰¾åˆ° {len(results3)} æ¡ç»“æœ")

    # 7. æµ‹è¯•è·å–æœ€è¿‘æ–°é—»
    logger.info("\nğŸ“° æµ‹è¯•è·å–æœ€è¿‘æ–°é—»...")
    recent = await db.get_recent_news(limit=5)
    logger.info(f"âœ… æœ€è¿‘ {len(recent)} æ¡æ–°é—»:")
    for news in recent:
        logger.info(f"   - {news.title} ({news.created_at})")

    # 8. æµ‹è¯•æ›´æ–°æ–°é—»å†…å®¹
    logger.info("\nâœï¸ æµ‹è¯•æ›´æ–°æ–°é—»å†…å®¹...")
    success = await db.update_news_content(
        url=news1.url,
        content="æ›´æ–°åçš„å®Œæ•´å†…å®¹...",
        html_content="<p>æ›´æ–°åçš„HTMLå†…å®¹</p>",
    )
    logger.info(f"{'âœ… æ›´æ–°æˆåŠŸ' if success else 'âŒ æ›´æ–°å¤±è´¥'}")

    # 9. æµ‹è¯•æ›´æ–°äº‹ä»¶åç§°
    logger.info("\nğŸ·ï¸ æµ‹è¯•æ›´æ–°äº‹ä»¶åç§°...")
    success = await db.update_event_name(news2.url, "æ¬§å† è”èµ›2026")
    logger.info(f"{'âœ… äº‹ä»¶åç§°æ›´æ–°æˆåŠŸ' if success else 'âŒ æ›´æ–°å¤±è´¥'}")

    # 10. æµ‹è¯•æ‰¹é‡æ›´æ–°äº‹ä»¶åç§°
    logger.info("\nğŸ“¦ æµ‹è¯•æ‰¹é‡æ›´æ–°äº‹ä»¶åç§°...")
    urls = [f"https://example.com/test/{i}" for i in range(3, 6)]
    batch_stats = await db.batch_update_event_name(urls, "æµ‹è¯•äº‹ä»¶2026")
    logger.info(f"âœ… æ‰¹é‡æ›´æ–°äº‹ä»¶åç§°: {batch_stats}")

    # 11. æµ‹è¯•è·å–ç»Ÿè®¡ä¿¡æ¯
    logger.info("\nğŸ“Š æµ‹è¯•è·å–ç»Ÿè®¡ä¿¡æ¯...")
    stats = await db.get_stats()
    logger.info("âœ… ç»Ÿè®¡ä¿¡æ¯:")
    logger.info(f"   æ€»æ•°: {stats['total']}")
    logger.info(f"   æœ€è¿‘7å¤©æ–°å¢: {stats['recent_week']}")
    logger.info(f"   æŒ‰æ¥æºåˆ†å¸ƒ: {stats['by_source']}")

    # 12. æµ‹è¯•åˆ é™¤æ–°é—»
    logger.info("\nğŸ—‘ï¸ æµ‹è¯•åˆ é™¤æ–°é—»...")
    delete_url = "https://example.com/test/3"
    success = await db.delete_news(delete_url)
    logger.info(f"{'âœ… åˆ é™¤æˆåŠŸ' if success else 'âŒ åˆ é™¤å¤±è´¥'}: {delete_url}")

    # 13. å†æ¬¡è·å–ç»Ÿè®¡ä¿¡æ¯éªŒè¯åˆ é™¤
    logger.info("\nğŸ“Š éªŒè¯åˆ é™¤åçš„ç»Ÿè®¡ä¿¡æ¯...")
    stats_after = await db.get_stats()
    logger.info(f"âœ… åˆ é™¤åæ€»æ•°: {stats_after['total']} (ä¹‹å‰: {stats['total']})")

    # 14. å…³é—­æ•°æ®åº“è¿æ¥
    logger.info("\nğŸ”’ å…³é—­æ•°æ®åº“è¿æ¥...")
    await db.close()
    logger.info("âœ… æµ‹è¯•å®Œæˆï¼")


async def test_storage_tools():
    """æµ‹è¯•å­˜å‚¨å·¥å…·å‡½æ•°"""
    from mcp_server.news_storage.tools.storage_tools import (
        save_news_tool,
        get_news_by_url_tool,
        search_news_tool,
        get_recent_news_tool,
    )

    logger.info("\nğŸ”§ æµ‹è¯•å­˜å‚¨å·¥å…·å‡½æ•°...")

    # æµ‹è¯•ä¿å­˜æ–°é—»å·¥å…·
    result = json.loads(
        await save_news_tool(
            title="å·¥å…·å‡½æ•°æµ‹è¯•æ–°é—»",
            url="https://example.com/tool-test",
            summary="è¿™æ˜¯é€šè¿‡å·¥å…·å‡½æ•°ä¿å­˜çš„æ–°é—»",
            source="æµ‹è¯•æº",
            keywords='["æµ‹è¯•", "å·¥å…·"]',
            tags='["æµ‹è¯•"]',
        )
    )
    logger.info(f"âœ… ä¿å­˜å·¥å…·ç»“æœ: {result}")

    # æµ‹è¯•è·å–æ–°é—»å·¥å…·
    result = json.loads(await get_news_by_url_tool("https://example.com/tool-test"))
    logger.info(f"âœ… è·å–å·¥å…·ç»“æœ: found={result.get('found')}")

    # æµ‹è¯•æœç´¢å·¥å…·
    result = json.loads(await search_news_tool(search="æµ‹è¯• å·¥å…·", limit=10))
    logger.info(f"âœ… æœç´¢å·¥å…·ç»“æœ: count={result.get('count')}")

    # æµ‹è¯•æœ€è¿‘æ–°é—»å·¥å…·
    result = json.loads(await get_recent_news_tool(limit=3))
    logger.info(f"âœ… æœ€è¿‘æ–°é—»å·¥å…·ç»“æœ: count={result.get('count')}")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logger.remove()
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO",
    )

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_database())
    asyncio.run(test_storage_tools())
