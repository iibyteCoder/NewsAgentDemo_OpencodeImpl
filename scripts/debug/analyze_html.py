"""
é€šç”¨HTMLæ¸…æ´—å·¥å…· - ç§»é™¤script/css/headerç­‰æ ‡ç­¾ï¼Œä¿ç•™bodyå†…å®¹
"""

import sys
import io
from bs4 import BeautifulSoup
from pathlib import Path

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


def clean_html(html_file: str, engine_name: str):
    """æ¸…æ´—HTMLæ–‡ä»¶ - ç§»é™¤script/css/headerç­‰æ ‡ç­¾ï¼Œä¿ç•™bodyå†…å®¹"""
    print(f"\n{'='*60}")
    print(f"æ¸…æ´— {engine_name} HTMLæ–‡ä»¶")
    print(f"{'='*60}")

    with open(html_file, 'r', encoding='utf-8') as f:
        html = f.read()

    soup = BeautifulSoup(html, 'html.parser')

    # åªç§»é™¤çœŸæ­£çš„å™ªéŸ³æ ‡ç­¾ï¼ˆä¸å½±å“å†…å®¹çš„å…³é”®ä¿¡æ¯ï¼‰
    # script/style: JavaScriptå’ŒCSSæ ·å¼
    # noscript: æ— è„šæœ¬æ—¶çš„æ›¿ä»£å†…å®¹
    # iframe/svg/canvas/video/audio: å¤šåª’ä½“å’ŒåµŒå…¥å†…å®¹
    tags_to_remove = ['script', 'style', 'noscript', 'iframe',
                      'svg', 'canvas', 'video', 'audio']

    removed_count = 0
    for tag_name in tags_to_remove:
        for tag in soup.find_all(tag_name):
            tag.decompose()
            removed_count += 1

    # ä¿å­˜æ¸…æ´—åçš„HTMLï¼ˆåªä¿ç•™bodyå†…å®¹ï¼‰
    if soup.body:
        cleaned_html = str(soup.body)
    else:
        cleaned_html = str(soup)

    # ä¿å­˜æ¸…æ´—åçš„æ–‡ä»¶
    output_file = Path("search_engine_demos") / f"{engine_name}_cleaned.html"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(cleaned_html)

    # ç»Ÿè®¡ä¿¡æ¯
    original_size = len(html)
    cleaned_size = len(cleaned_html)
    reduction_rate = (1 - cleaned_size / original_size) * 100

    print(f"âœ… æ¸…æ´—å®Œæˆ")
    print(f"   ç§»é™¤äº† {removed_count} ä¸ªæ ‡ç­¾")
    print(f"   åŸå§‹å¤§å°: {original_size:,} å­—ç¬¦")
    print(f"   æ¸…æ´—åå¤§å°: {cleaned_size:,} å­—ç¬¦")
    print(f"   æ¸…ç†ç‡: {reduction_rate:.1f}%")
    print(f"   å·²ä¿å­˜åˆ°: {output_file}")

    # åˆ†ææ¸…æ´—åçš„åŸºæœ¬ç»“æ„
    soup_cleaned = BeautifulSoup(cleaned_html, 'html.parser')
    print("\nğŸ“Š æ¸…æ´—ååŸºæœ¬ç»Ÿè®¡:")
    print(f"   é“¾æ¥æ•°é‡: {len(soup_cleaned.find_all('a', href=True))}")
    print(f"   æ ‡é¢˜æ•°é‡(h1-h4): {len(soup_cleaned.find_all(['h1', 'h2', 'h3', 'h4']))}")
    print(f"   å›¾ç‰‡æ•°é‡: {len(soup_cleaned.find_all('img'))}")
    print(f"   divæ•°é‡: {len(soup_cleaned.find_all('div'))}")
    print(f"   pæ•°é‡: {len(soup_cleaned.find_all('p'))}")


def main():
    """ä¸»å‡½æ•°"""
    demo_dir = Path("search_engine_demos")

    engines = {
        "ç™¾åº¦": "ç™¾åº¦_news.html",
        "å¿…åº”": "å¿…åº”_news.html",
        "è°·æ­Œ": "è°·æ­Œ_news.html",
        "æœç‹—": "æœç‹—_news.html",
    }

    for engine_name, filename in engines.items():
        html_file = demo_dir / filename

        if not html_file.exists():
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {html_file}")
            continue

        try:
            clean_html(str(html_file), engine_name)
        except Exception as e:
            print(f"âŒ æ¸…æ´—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
