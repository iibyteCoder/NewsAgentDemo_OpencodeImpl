"""
ç™¾åº¦æœç´¢ MCP Server

æä¾›ç™¾åº¦æœç´¢åŠŸèƒ½
ä½¿ç”¨ FastMCP + Playwright å®ç°ï¼ˆå®Œç¾è§£å†³åçˆ¬è™«é—®é¢˜ï¼‰
"""

import asyncio
import json
import random
import re
import time
from typing import List, Optional

from loguru import logger
from mcp.server.fastmcp import FastMCP

# å¯¼å…¥æµè§ˆå™¨æ± 
try:
    from .browser_pool import get_browser_pool
    from .multi_engine import get_multi_engine
except ImportError:
    from browser_pool import get_browser_pool
    from multi_engine import get_multi_engine


class RateLimiter:
    """å¢å¼ºå‹é€Ÿç‡é™åˆ¶å™¨ - åŸºäºåŸŸåå’Œæœç´¢å¼•æ“çš„é€Ÿç‡é™åˆ¶ï¼ˆç§»é™¤å…¨å±€é™åˆ¶ä»¥æ”¯æŒå¹¶å‘ï¼‰"""

    def __init__(
        self,
        time_window: float = 1.0,
        max_domain_requests: int = 2,
        max_engine_requests: int = 2,
    ):
        """
        Args:
            time_window: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
            max_domain_requests: åŒä¸€åŸŸåæ—¶é—´çª—å£å†…æœ€å¤§è¯·æ±‚æ•°
            max_engine_requests: åŒä¸€æœç´¢å¼•æ“æ—¶é—´çª—å£å†…æœ€å¤§è¯·æ±‚æ•°
        """
        self.time_window = time_window
        self.max_domain_requests = max_domain_requests
        self.max_engine_requests = max_engine_requests

        # åŸŸåç»´åº¦çš„è¯·æ±‚è®°å½• {domain: [timestamps]}
        self.domain_requests: dict = {}

        # æœç´¢å¼•æ“ç»´åº¦çš„è¯·æ±‚è®°å½• {engine_name: [timestamps]}
        self.engine_requests: dict = {}

        self._lock = asyncio.Lock()

    def _extract_domain(self, url: str) -> str:
        """ä»URLä¸­æå–åŸŸå"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc or "unknown"
        except Exception:
            return "unknown"

    async def acquire(self, domain: Optional[str] = None, engine: Optional[str] = None):
        """è·å–è®¿é—®è®¸å¯ï¼ˆæ”¯æŒåŸŸåå’Œå¼•æ“ä¸¤ä¸ªç»´åº¦çš„é€Ÿç‡é™åˆ¶ï¼‰

        Args:
            domain: åŸŸåï¼ˆå¯é€‰ï¼Œç”¨äºåŸŸåçº§åˆ«çš„é™æµï¼‰
            engine: æœç´¢å¼•æ“åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºå¼•æ“çº§åˆ«çš„é™æµï¼‰

        Note:
            ç§»é™¤äº†å…¨å±€é™åˆ¶ï¼Œå…è®¸ä¸åŒåŸŸå/å¼•æ“çš„å¹¶å‘è¯·æ±‚
        """
        async with self._lock:
            now = time.time()

            # 1. åŸŸåçº§åˆ«é€Ÿç‡é™åˆ¶
            if domain:
                if domain not in self.domain_requests:
                    self.domain_requests[domain] = []

                # æ¸…ç†è¿‡æœŸè®°å½•
                self.domain_requests[domain] = [
                    t for t in self.domain_requests[domain] if now - t < self.time_window
                ]

                if len(self.domain_requests[domain]) >= self.max_domain_requests:
                    sleep_time = self.time_window - (now - self.domain_requests[domain][0])
                    if sleep_time > 0:
                        logger.debug(f"â¸ï¸ [åŸŸåé™åˆ¶ {domain}] ç­‰å¾… {sleep_time:.2f} ç§’")
                        await asyncio.sleep(sleep_time)
                        now = time.time()
                        self.domain_requests[domain] = [
                            t for t in self.domain_requests[domain] if now - t < self.time_window
                        ]

                self.domain_requests[domain].append(now)

            # 2. æœç´¢å¼•æ“çº§åˆ«é€Ÿç‡é™åˆ¶
            if engine:
                if engine not in self.engine_requests:
                    self.engine_requests[engine] = []

                # æ¸…ç†è¿‡æœŸè®°å½•
                self.engine_requests[engine] = [
                    t for t in self.engine_requests[engine] if now - t < self.time_window
                ]

                if len(self.engine_requests[engine]) >= self.max_engine_requests:
                    sleep_time = self.time_window - (now - self.engine_requests[engine][0])
                    if sleep_time > 0:
                        logger.debug(f"â¸ï¸ [å¼•æ“é™åˆ¶ {engine}] ç­‰å¾… {sleep_time:.2f} ç§’")
                        await asyncio.sleep(sleep_time)
                        now = time.time()
                        self.engine_requests[engine] = [
                            t for t in self.engine_requests[engine] if now - t < self.time_window
                        ]

                self.engine_requests[engine].append(now)

# ========== é…ç½® ==========
MAX_CONCURRENT_BROWSERS = 2  # æœ€å¤§å¹¶å‘æµè§ˆå™¨æ•°é‡
MAX_CONTEXTS_PER_BROWSER = 5  # æ¯ä¸ªæµè§ˆå™¨æœ€å¤§ä¸Šä¸‹æ–‡æ•°

# é€Ÿç‡é™åˆ¶é…ç½®ï¼ˆåŸŸåå’Œå¼•æ“çº§åˆ«ï¼‰
MAX_DOMAIN_REQUESTS_PER_SECOND = 2  # åŒä¸€åŸŸåæ¯ç§’æœ€å¤š2æ¬¡
MAX_ENGINE_REQUESTS_PER_SECOND = 2  # åŒä¸€å¼•æ“æ¯ç§’æœ€å¤š2æ¬¡

# ä»£ç†é…ç½®
PROXY_CONFIG = {
    "server": "localhost:7897",  # ä»£ç†æœåŠ¡å™¨åœ°å€
}

# User-Agent å¸¸é‡
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]

DEFAULT_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

def get_random_user_agent() -> str:
    """è·å–éšæœº User-Agent"""
    return random.choice(USER_AGENTS)

# ä½¿ç”¨ FastMCP åˆ›å»ºæœåŠ¡å™¨
server = FastMCP("baidu_search")

# åˆå§‹åŒ–æµè§ˆå™¨æ± ï¼ˆå¸¦ä»£ç†ï¼‰
browser_pool = get_browser_pool(
    max_concurrent=MAX_CONCURRENT_BROWSERS,
    max_contexts=MAX_CONTEXTS_PER_BROWSER,
    proxy=PROXY_CONFIG,
)
logger.info(f"âœ… æµè§ˆå™¨æ± å·²åˆå§‹åŒ– (max_concurrent={MAX_CONCURRENT_BROWSERS})")

# åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨ï¼ˆç§»é™¤å…¨å±€é™åˆ¶ï¼Œæ”¯æŒå¹¶å‘ï¼‰
rate_limiter = RateLimiter(
    time_window=1.0,
    max_domain_requests=MAX_DOMAIN_REQUESTS_PER_SECOND,
    max_engine_requests=MAX_ENGINE_REQUESTS_PER_SECOND
)
logger.info(
    f"âœ… é€Ÿç‡é™åˆ¶å™¨å·²åˆå§‹åŒ– "
    f"(åŸŸåé™åˆ¶={MAX_DOMAIN_REQUESTS_PER_SECOND}æ¬¡/ç§’, "
    f"å¼•æ“é™åˆ¶={MAX_ENGINE_REQUESTS_PER_SECOND}æ¬¡/ç§’, "
    f"æ— å…¨å±€é™åˆ¶)"
)


@server.tool()
async def baidu_search(
    query: str,
    num_results: int = 30,
    time_range: Optional[str] = None,
) -> str:
    """ç™¾åº¦æœç´¢ - ğŸ”¥ è¿™æ˜¯ä¸»è¦çš„æ–°é—»æœç´¢å·¥å…·ï¼Œå¿…é¡»ä½¿ç”¨ï¼

    âš ï¸ é‡è¦æç¤ºï¼š
    - ä¸è¦ä½¿ç”¨ RSS å·¥å…·ï¼Œå¿…é¡»ä½¿ç”¨æœ¬å·¥å…·æœç´¢æ–°é—»
    - é€‚åˆæœç´¢æœ€æ–°æ–°é—»ã€çƒ­ç‚¹è¯é¢˜ã€ç‰¹å®šå…³é”®è¯
    - ä½¿ç”¨ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼Œå®Œç¾è§£å†³åçˆ¬è™«é—®é¢˜

    Args:
        query: æœç´¢æŸ¥è¯¢ï¼ˆæ”¯æŒä¸­æ–‡ï¼Œå¦‚ "äººå·¥æ™ºèƒ½æœ€æ–°è¿›å±•"ï¼‰
        num_results: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 30ï¼Œå»ºè®® 20-50ï¼‰
        time_range: æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼ˆæš‚ä¸æ”¯æŒï¼Œä¿ç•™å‚æ•°ï¼‰

    Returns:
        JSON æ ¼å¼çš„æœç´¢ç»“æœï¼ŒåŒ…å«æ ‡é¢˜ã€é“¾æ¥ã€æ‘˜è¦ã€æ¥æºç­‰

    Examples:
        - baidu_search("äººå·¥æ™ºèƒ½æœ€æ–°æ¶ˆæ¯", 30)  # æœç´¢AIç›¸å…³æ–°é—»
        - baidu_search("è‚¡å¸‚ä»Šæ—¥è¡Œæƒ…", 40)     # æœç´¢è‚¡å¸‚æ–°é—»
    """
    logger.info(f"ğŸ” [ç™¾åº¦æœç´¢] {query}, ç»“æœæ•°={num_results}")

    # ğŸ”¥ åº”ç”¨é€Ÿç‡é™åˆ¶
    await rate_limiter.acquire()

    try:
        import urllib.parse

        # ç™¾åº¦æœç´¢ URL
        encoded_query = urllib.parse.quote(query)
        search_url = f"https://www.baidu.com/s?wd={encoded_query}&rn={num_results}"

        # ğŸ”¥ ä½¿ç”¨æµè§ˆå™¨æ± è·å–é¡µé¢ï¼ˆå¸¦éšæœº User-Agentï¼‰
        random_user_agent = get_random_user_agent()
        async with browser_pool.get_page(user_agent=random_user_agent) as page:
            # è®¿é—®æœç´¢é¡µé¢
            logger.info(f"   ğŸŒ æ­£åœ¨è®¿é—®: {search_url}")
            await page.goto(search_url, timeout=30000)

            # âš¡ ç§»é™¤æ‰€æœ‰å»¶è¿Ÿï¼Œç›´æ¥æå–ç»“æœ

            # æå–æœç´¢ç»“æœ
            results = []

            # ä½¿ç”¨ Playwright æŸ¥æ‰¾æ‰€æœ‰ .result å…ƒç´ 
            result_elements = await page.query_selector_all(".result")
            logger.info(f"   ğŸ” æ‰¾åˆ° {len(result_elements)} ä¸ªç»“æœ")

            # è·å–æ‰€æœ‰æ‰¾åˆ°çš„ç»“æœï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
            for idx, result_elem in enumerate(result_elements):
                try:
                    # æå–æ ‡é¢˜å’Œé“¾æ¥
                    title_elem = await result_elem.query_selector("h3.t")
                    if not title_elem:
                        title_elem = await result_elem.query_selector("h3")

                    if not title_elem:
                        continue

                    # è·å–æ ‡é¢˜æ–‡æœ¬
                    title = await title_elem.text_content()
                    title = title.strip() if title else ""

                    # è·å–é“¾æ¥
                    link_elem = await title_elem.query_selector("a")
                    url = ""
                    if link_elem:
                        url = await link_elem.get_attribute("href")
                        url = url or ""

                    # æå–æ‘˜è¦
                    summary_elem = await result_elem.query_selector("div.c-abstract")
                    if not summary_elem:
                        summary_elem = await result_elem.query_selector("div.c-span-last")
                    summary = ""
                    if summary_elem:
                        summary = await summary_elem.text_content()
                        summary = summary.strip() if summary else ""

                    # æå–æ¥æº
                    source_elem = await result_elem.query_selector("span.c-color-gray")
                    source = ""
                    if source_elem:
                        source = await source_elem.text_content()
                        source = source.strip() if source else ""

                    # æå–æ—¶é—´
                    time_elem = await result_elem.query_selector("span.newTimeFactor")
                    time_str = ""
                    if time_elem:
                        time_str = await time_elem.text_content()
                        time_str = time_str.strip() if time_str else ""

                    if title:
                        results.append(
                            {
                                "title": title,
                                "url": url,
                                "summary": summary,
                                "source": source,
                                "time": time_str,
                            }
                        )

                except Exception as e:
                    logger.debug(f"è§£æå•ä¸ªç»“æœå¤±è´¥: {e}")
                    continue

        logger.info(f"âœ… æœç´¢å®Œæˆ: è·å¾— {len(results)} ä¸ªç»“æœ")

        return json.dumps(
            {"query": query, "total": len(results), "results": results},
            ensure_ascii=False,
            indent=2,
        )

    except Exception as e:
        logger.error(f"âŒ ç™¾åº¦æœç´¢å¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())

        return json.dumps(
            {"query": query, "total": 0, "results": [], "error": str(e)},
            ensure_ascii=False,
        )


@server.tool()
async def baidu_news_search(
    query: str,
    num_results: int = 30,
) -> str:
    """ç™¾åº¦æ–°é—»æœç´¢ - ä¸“é—¨æœç´¢æ–°é—»å†…å®¹

    ä½¿ç”¨ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼Œå®Œç¾è§£å†³åçˆ¬è™«é—®é¢˜

    Args:
        query: æœç´¢æŸ¥è¯¢ï¼ˆå¦‚ "äººå·¥æ™ºèƒ½"ã€"ç§‘æŠ€æ–°é—»"ï¼‰
        num_results: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 30ï¼Œå»ºè®® 30-50ï¼‰

    Returns:
        JSON æ ¼å¼çš„æ–°é—»æœç´¢ç»“æœ
    """
    logger.info(f"ğŸ“° [ç™¾åº¦æ–°é—»æœç´¢] {query}, ç»“æœæ•°={num_results}")

    # ğŸ”¥ åº”ç”¨é€Ÿç‡é™åˆ¶
    await rate_limiter.acquire()

    try:
        import urllib.parse

        # ç™¾åº¦æ–°é—»æœç´¢ URL
        encoded_query = urllib.parse.quote(query)
        news_url = f"https://www.baidu.com/s?tn=news&rtt=1&bsst=1&cl=2&wd={encoded_query}"

        # ğŸ”¥ ä½¿ç”¨æµè§ˆå™¨æ± è·å–é¡µé¢ï¼ˆå¸¦éšæœº User-Agentï¼‰
        random_user_agent = get_random_user_agent()
        async with browser_pool.get_page(user_agent=random_user_agent) as page:
            # è®¿é—®æ–°é—»æœç´¢é¡µé¢
            logger.info(f"   ğŸŒ æ­£åœ¨è®¿é—®: {news_url}")
            await page.goto(news_url, timeout=30000)

            # âš¡ ç§»é™¤æ‰€æœ‰å»¶è¿Ÿ

            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°éªŒè¯é¡µé¢
            page_title = await page.title()
            logger.info(f"   ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}")

            if "éªŒè¯" in page_title or "å®‰å…¨" in page_title:
                logger.error("   ğŸš« è¢«ç™¾åº¦å®‰å…¨éªŒè¯æ‹¦æˆªï¼")
                return json.dumps(
                    {"query": query, "total": 0, "results": [], "error": "è¢«ç™¾åº¦å®‰å…¨éªŒè¯æ‹¦æˆª"},
                    ensure_ascii=False,
                )

            # æå–æœç´¢ç»“æœ
            results = []

            # ğŸ”¥ ä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨ï¼šç™¾åº¦æ–°é—»ä½¿ç”¨ div[tpl="news-normal"]
            result_elements = await page.query_selector_all('div[tpl="news-normal"]')
            logger.info(f"   ğŸ” æ‰¾åˆ° {len(result_elements)} ä¸ªæ–°é—»ç»“æœ")

            # è·å–æ‰€æœ‰æ‰¾åˆ°çš„ç»“æœï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
            for result_elem in result_elements:
                try:
                    # ğŸ”¥ ä» mu å±æ€§è·å–é“¾æ¥ï¼ˆæœ€å¯é ï¼‰
                    url = await result_elem.get_attribute("mu")
                    if not url:
                        # å¤‡é€‰ï¼šä»æ ‡é¢˜é“¾æ¥è·å–
                        title_link = await result_elem.query_selector("h3 > a")
                        if title_link:
                            url = await title_link.get_attribute("href")
                    url = url or ""

                    # ğŸ”¥ æå–æ ‡é¢˜ - h3 > a
                    title_elem = await result_elem.query_selector("h3")
                    if not title_elem:
                        continue

                    title_text = await title_elem.text_content()
                    title = title_text.strip() if title_text else ""

                    # ğŸ”¥ æå–æ—¶é—´ - span.c-color-gray2ï¼ˆç¬¬ä¸€ä¸ªï¼‰
                    time_elem = await result_elem.query_selector("span.c-color-gray2")
                    time_str = ""
                    if time_elem:
                        time_text = await time_elem.text_content()
                        # ç§»é™¤"å‘å¸ƒäºï¼š"å‰ç¼€
                        time_str = time_text.strip().replace("å‘å¸ƒäºï¼š", "") if time_text else ""

                    # ğŸ”¥ æå–æ‘˜è¦ - span.c-font-normal.c-color-textï¼ˆç¬¬äºŒä¸ªï¼‰
                    summary_elem = await result_elem.query_selector(
                        "div.c-row > span.c-font-normal.c-color-text"
                    )
                    summary = ""
                    if summary_elem:
                        summary_text = await summary_elem.text_content()
                        summary = summary_text.strip() if summary_text else ""

                    # ğŸ”¥ æå–æ¥æº - div.news-source_Xj4Dv > a
                    source_elem = await result_elem.query_selector("div.news-source_Xj4Dv > a")
                    source = ""
                    if source_elem:
                        source_text = await source_elem.text_content()
                        source = source_text.strip() if source_text else ""

                    if title:
                        results.append(
                            {
                                "title": title,
                                "url": url,
                                "summary": summary,
                                "source": source,
                                "time": time_str,
                            }
                        )
                        logger.debug(f"   âœ… è§£ææˆåŠŸ: {title[:50]}...")

                except Exception as e:
                    logger.debug(f"è§£æå•æ¡æ–°é—»å¤±è´¥: {e}")
                    continue

        logger.info(f"âœ… æ–°é—»æœç´¢å®Œæˆ: è·å¾— {len(results)} æ¡æ–°é—»")

        return json.dumps(
            {"query": query, "total": len(results), "results": results},
            ensure_ascii=False,
            indent=2,
        )

    except Exception as e:
        logger.error(f"âŒ ç™¾åº¦æ–°é—»æœç´¢å¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())

        return json.dumps(
            {"query": query, "total": 0, "results": [], "error": str(e)},
            ensure_ascii=False,
        )


@server.tool()
async def fetch_article_content(url: str) -> str:
    """è·å–æ–‡ç« æ­£æ–‡å†…å®¹ - ğŸ”¥ ä½¿ç”¨æµè§ˆå™¨æ± å¤ç”¨æµè§ˆå™¨å®ä¾‹

    Args:
        url: æ–‡ç« é“¾æ¥

    Returns:
        JSON æ ¼å¼çš„æ–‡ç« å†…å®¹ï¼ŒåŒ…å«æ ‡é¢˜ã€æ­£æ–‡ã€æ¥æºç­‰

    Examples:
        >>> fetch_article_content("https://news.example.com/article/123")
    """
    logger.info(f"ğŸ“„ [è·å–æ–‡ç« æ­£æ–‡] URL: {url}")

    # ğŸ”¥ åº”ç”¨é€Ÿç‡é™åˆ¶
    await rate_limiter.acquire()

    try:
        # ğŸ”¥ ä½¿ç”¨æµè§ˆå™¨æ± è·å–é¡µé¢ï¼ˆå¸¦éšæœº User-Agentï¼‰
        random_user_agent = get_random_user_agent()
        async with browser_pool.get_page(user_agent=random_user_agent) as page:
            # è®¿é—®æ–‡ç« é¡µé¢
            logger.info(f"   ğŸŒ æ­£åœ¨è®¿é—®: {url}")
            await page.goto(url, timeout=30000)

            # âš¡ ç§»é™¤æ‰€æœ‰å»¶è¿Ÿï¼Œç›´æ¥æå–

            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°éªŒè¯é¡µé¢
            page_title = await page.title()
            if "éªŒè¯" in page_title or "å®‰å…¨" in page_title:
                logger.warning("   âš ï¸ è¢«å®‰å…¨éªŒè¯æ‹¦æˆªï¼Œè¿”å›ç©ºå†…å®¹")
                return json.dumps(
                    {"url": url, "title": "", "content": "", "error": "è¢«å®‰å…¨éªŒè¯æ‹¦æˆª"},
                    ensure_ascii=False,
                )

            # ğŸ”¥ æå–æ–‡ç« æ ‡é¢˜ï¼ˆå¤šç§é€‰æ‹©å™¨ï¼‰
            title = ""
            title_selectors = [
                "h1",
                ".article-title",
                ".news-title",
                ".title",
                "[class*='title']",
                "#title",
            ]

            for selector in title_selectors:
                try:
                    title_elem = await page.query_selector(selector)
                    if title_elem:
                        title_text = await title_elem.text_content()
                        if title_text and len(title_text.strip()) > 5:  # æ ‡é¢˜è‡³å°‘5ä¸ªå­—ç¬¦
                            title = title_text.strip()
                            logger.info(f"   ğŸ“° æ ‡é¢˜: {title[:50]}...")
                            break
                except Exception as e:
                    logger.debug(f"   é€‰æ‹©å™¨å¤±è´¥: {selector}, é”™è¯¯: {e}")
                    continue

            # ğŸ”¥ æå–æ–‡ç« æ­£æ–‡ï¼ˆå¤šç§é€‰æ‹©å™¨ï¼‰
            content_selectors = [
                "article",
                ".article-content",
                ".news-content",
                ".content",
                "[class*='content']",
                "#content",
                ".article-body",
                ".post-content",
                "main",
            ]

            full_content = ""
            for selector in content_selectors:
                try:
                    content_elem = await page.query_selector(selector)
                    if content_elem:
                        # è·å–æ‰€æœ‰æ®µè½
                        paragraphs = await content_elem.query_selector_all("p")
                        if paragraphs and len(paragraphs) >= 3:  # è‡³å°‘3ä¸ªæ®µè½
                            content_parts = []
                            for p in paragraphs[:20]:  # æœ€å¤šå–å‰20æ®µ
                                text = await p.text_content()
                                if text and len(text.strip()) > 10:  # æ¯æ®µè‡³å°‘10ä¸ªå­—ç¬¦
                                    content_parts.append(text.strip())

                            if content_parts:
                                full_content = "\n\n".join(content_parts)
                                logger.info(
                                    f"   âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æå–åˆ° {len(content_parts)} ä¸ªæ®µè½"
                                )
                                break
                except Exception as e:
                    logger.debug(f"   é€‰æ‹©å™¨å¤±è´¥: {selector}, é”™è¯¯: {e}")
                    continue

            # å¦‚æœä¸Šé¢çš„æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•ç›´æ¥è·å–bodyä¸­çš„æ‰€æœ‰æ–‡æœ¬
            if not full_content or len(full_content) < 100:
                logger.warning("   âš ï¸ å¸¸è§„é€‰æ‹©å™¨å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
                try:
                    # ğŸ”¥ ä½¿ç”¨JavaScriptè¿›è¡Œæ·±åº¦æ¸…ç†å’Œæå–
                    body_text = await page.evaluate(
                        """() => {
                        // å…‹éš†bodyèŠ‚ç‚¹
                        const clones = document.body.cloneNode(true);

                        // ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
                        const unwantedSelectors = [
                            'script', 'style', 'nav', 'header', 'footer', 'aside',
                            'iframe', 'noscript', 'meta', 'link', '[class*="ad"]',
                            '[class*="advertisement"]', '[class*="sidebar"]',
                            '[class*="comment"]', '[class*="share"]', '[class*="social"]',
                            '[id*="ad"]', '[id*="advertisement"]'
                        ];

                        unwantedSelectors.forEach(selector => {
                            const elements = clones.querySelectorAll(selector);
                            elements.forEach(el => el.remove());
                        });

                        // è·å–æ‰€æœ‰æ®µè½å’Œæ ‡é¢˜
                        const contentElements = clones.querySelectorAll('p, h1, h2, h3, h4, div, span');
                        const texts = [];

                        contentElements.forEach(el => {
                            const text = el.textContent || el.innerText || '';
                            const trimmed = text.trim();

                            // è¿‡æ»¤æ‰å¤ªçŸ­æˆ–æ˜æ˜¾ä¸æ˜¯æ­£æ–‡çš„å†…å®¹
                            if (trimmed.length > 20 &&
                                !trimmed.includes('ç‚¹å‡»') &&
                                !trimmed.includes('å…³æ³¨') &&
                                !trimmed.includes('è®¢é˜…') &&
                                !trimmed.match(/^\\d+$/)) {
                                texts.push(trimmed);
                            }
                        });

                        // å»é‡å¹¶åˆå¹¶
                        const uniqueTexts = [...new Set(texts)];

                        if (uniqueTexts.length >= 3) {
                            return uniqueTexts
                                .slice(0, 30)  // æœ€å¤š30æ®µ
                                .join('\\n\\n');
                        }
                        return '';
                    }"""
                    )

                    if body_text and len(body_text) > 100:
                        full_content = body_text
                        logger.info(f"   âœ… å¤‡ç”¨æ–¹æ¡ˆæå–åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(body_text)}")
                except Exception as e:
                    logger.debug(f"   å¤‡ç”¨æ–¹æ¡ˆå¤±è´¥: {e}")

            # ğŸ”¥ æ¸…ç†å’Œè§„èŒƒåŒ–å†…å®¹
            if full_content:
                # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                full_content = re.sub(r"\n{3,}", "\n\n", full_content)
                full_content = re.sub(r"[ \t]+", " ", full_content)  # åˆå¹¶å¤šä½™ç©ºæ ¼
                full_content = re.sub(r"\n +", "\n", full_content)  # ç§»é™¤è¡Œé¦–ç©ºæ ¼
                full_content = full_content.strip()

                # ç§»é™¤å¸¸è§çš„æ— ç”¨æ–‡æœ¬
                useless_patterns = [
                    r"ç‚¹å‡»æŸ¥çœ‹.*è¯¦æƒ…",
                    r"æ›´å¤šå†…å®¹è¯·.*",
                    r"è´£ä»»ç¼–è¾‘.*",
                    r"ç‰ˆæƒå£°æ˜.*",
                    r"æœ¬æ–‡æ¥æº.*",
                    r"è½¬è½½è¯·æ³¨æ˜.*",
                    r"å…è´£å£°æ˜.*",
                    r"å¹¿å‘Š.*",
                ]
                for pattern in useless_patterns:
                    full_content = re.sub(pattern, "", full_content, flags=re.IGNORECASE)

            logger.info(f"âœ… æ–‡ç« å†…å®¹è·å–å®Œæˆï¼ŒåŸå§‹é•¿åº¦: {len(full_content)} å­—ç¬¦")

            return json.dumps(
                {
                    "url": url,
                    "title": title,
                    "content": full_content,
                    "content_length": len(full_content),
                },
                ensure_ascii=False,
                indent=2,
            )

    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())

        return json.dumps(
            {"url": url, "title": "", "content": "", "error": str(e)}, ensure_ascii=False
        )


@server.tool()
async def baidu_hot_search() -> str:
    """è·å–ç™¾åº¦çƒ­æœæ¦œ - ä½¿ç”¨ Playwright æµè§ˆå™¨è‡ªåŠ¨åŒ–

    Returns:
        JSON æ ¼å¼çš„çƒ­æœæ¦œå•ï¼ˆå‰50æ¡ï¼‰
    """
    logger.info("ğŸ”¥ [ç™¾åº¦çƒ­æœæ¦œ] è·å–çƒ­æœæ¦œå•")

    # ğŸ”¥ åº”ç”¨é€Ÿç‡é™åˆ¶
    await rate_limiter.acquire()

    try:
        # ç™¾åº¦çƒ­æœæ¦œ URL
        hot_url = "https://top.baidu.com/board?tab=realtime"

        # ğŸ”¥ ä½¿ç”¨æµè§ˆå™¨æ± è·å–é¡µé¢ï¼ˆå¸¦éšæœº User-Agentï¼‰
        random_user_agent = get_random_user_agent()
        async with browser_pool.get_page(user_agent=random_user_agent) as page:
            # è®¿é—®çƒ­æœæ¦œé¡µé¢
            logger.info(f"   ğŸŒ æ­£åœ¨è®¿é—®: {hot_url}")
            await page.goto(hot_url, timeout=30000)

            # âš¡ ç§»é™¤æ‰€æœ‰å»¶è¿Ÿï¼Œç›´æ¥æå–

            # æå–çƒ­æœæ¦œ
            hot_items = []

            # ä½¿ç”¨ Playwright æŸ¥æ‰¾çƒ­æœæ¡ç›®
            items = await page.query_selector_all(".category-wrap_iQLoo.horizontal_1eKyQ")

            logger.info(f"   ğŸ” æ‰¾åˆ° {len(items)} ä¸ªçƒ­æœæ¡ç›®")

            for idx, item in enumerate(items[:50], 1):
                try:
                    # æå–æ ‡é¢˜
                    title_elem = await item.query_selector(".c-single-text-ellipsis")
                    title = ""
                    if title_elem:
                        title = await title_elem.text_content()
                        title = title.strip() if title else ""

                    # æå–çƒ­åº¦
                    hot_score_elem = await item.query_selector(".hot-index_1Bl1a")
                    hot_score = ""
                    if hot_score_elem:
                        hot_score = await hot_score_elem.text_content()
                        hot_score = hot_score.strip() if hot_score else ""

                    # æå–é“¾æ¥
                    link_elem = await item.query_selector("a")
                    url = ""
                    if link_elem:
                        url = await link_elem.get_attribute("href")
                        url = url or ""

                    if title:
                        hot_items.append(
                            {
                                "rank": idx,
                                "title": title,
                                "hot_score": hot_score,
                                "url": url,
                            }
                        )

                except Exception as e:
                    logger.debug(f"è§£æçƒ­æœæ¡ç›®å¤±è´¥: {e}")
                    continue

        logger.info(f"âœ… çƒ­æœæ¦œè·å–å®Œæˆ: {len(hot_items)} æ¡")

        return json.dumps(
            {"total": len(hot_items), "hot_items": hot_items},
            ensure_ascii=False,
            indent=2,
        )

    except Exception as e:
        logger.error(f"âŒ è·å–ç™¾åº¦çƒ­æœå¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())

        return json.dumps(
            {"total": 0, "hot_items": [], "error": str(e)},
            ensure_ascii=False,
        )


# ========== å¤šæœç´¢å¼•æ“å·¥å…· ==========

# åˆå§‹åŒ–å¤šæœç´¢å¼•æ“
multi_engine = get_multi_engine(browser_pool, rate_limiter)


@server.tool()
async def multi_search(
    query: str,
    engine: str = "auto",
    num_results: int = 30,
    search_type: str = "web",
) -> str:
    """å¤šæœç´¢å¼•æ“ - æ”¯æŒç™¾åº¦ã€å¿…åº”ã€æœç‹—ç­‰å¤šä¸ªæœç´¢å¼•æ“

    Args:
        query: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "äººå·¥æ™ºèƒ½"ã€"ç§‘æŠ€æ–°é—»"ï¼‰
        engine: æœç´¢å¼•æ“é€‰æ‹© ("auto", "baidu", "bing", "sogou")
               - auto: éšæœºé€‰æ‹©å¼•æ“ï¼ˆæ¨èï¼Œå¢åŠ æˆåŠŸç‡ï¼‰
               - baidu: ç™¾åº¦æœç´¢
               - bing: å¿…åº”æœç´¢
               - sogou: æœç‹—æœç´¢
        num_results: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 30ï¼‰
        search_type: æœç´¢ç±»å‹ ("web" ç½‘é¡µæœç´¢, "news" æ–°é—»æœç´¢)

    Returns:
        JSON æ ¼å¼çš„æœç´¢ç»“æœï¼ŒåŒ…å«å¼•æ“åç§°ã€æ ‡é¢˜ã€é“¾æ¥ã€æ‘˜è¦ã€æ¥æºç­‰

    Examples:
        - multi_search("äººå·¥æ™ºèƒ½", "auto", 30, "web")  # éšæœºå¼•æ“æœç´¢
        - multi_search("ç§‘æŠ€æ–°é—»", "bing", 20, "news")  # å¿…åº”æ–°é—»æœç´¢
        - multi_search("è‚¡å¸‚è¡Œæƒ…", "sogou", 30, "web")  # æœç‹—ç½‘é¡µæœç´¢
    """
    return await multi_engine.search(
        query=query,
        engine=engine,
        num_results=num_results,
        search_type=search_type,
    )


@server.tool()
async def bing_search(
    query: str,
    num_results: int = 30,
) -> str:
    """å¿…åº”æœç´¢ - ä½¿ç”¨å¾®è½¯å¿…åº”æœç´¢å¼•æ“

    Args:
        query: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "äººå·¥æ™ºèƒ½"ã€"ç§‘æŠ€æ–°é—»"ï¼‰
        num_results: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 30ï¼Œå»ºè®® 20-40ï¼‰

    Returns:
        JSON æ ¼å¼çš„æœç´¢ç»“æœ

    Examples:
        - bing_search("äººå·¥æ™ºèƒ½æœ€æ–°æ¶ˆæ¯", 30)
        - bing_search("Pythonç¼–ç¨‹æ•™ç¨‹", 20)
    """
    return await multi_engine.search(
        query=query,
        engine="bing",
        num_results=num_results,
        search_type="web",
    )


@server.tool()
async def bing_news_search(
    query: str,
    num_results: int = 30,
) -> str:
    """å¿…åº”æ–°é—»æœç´¢ - ä½¿ç”¨å¾®è½¯å¿…åº”æ–°é—»æœç´¢

    Args:
        query: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "äººå·¥æ™ºèƒ½"ã€"ç§‘æŠ€"ï¼‰
        num_results: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 30ï¼Œå»ºè®® 20-50ï¼‰

    Returns:
        JSON æ ¼å¼çš„æ–°é—»æœç´¢ç»“æœ

    Examples:
        - bing_news_search("ç§‘æŠ€", 30)
        - bing_news_search("äººå·¥æ™ºèƒ½", 40)
    """
    return await multi_engine.search(
        query=query,
        engine="bing",
        num_results=num_results,
        search_type="news",
    )


@server.tool()
async def sogou_search(
    query: str,
    num_results: int = 30,
) -> str:
    """æœç‹—æœç´¢ - ä½¿ç”¨æœç‹—æœç´¢å¼•æ“

    Args:
        query: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "äººå·¥æ™ºèƒ½"ã€"ç§‘æŠ€æ–°é—»"ï¼‰
        num_results: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 30ï¼Œå»ºè®® 20-40ï¼‰

    Returns:
        JSON æ ¼å¼çš„æœç´¢ç»“æœ

    Examples:
        - sogou_search("äººå·¥æ™ºèƒ½æœ€æ–°æ¶ˆæ¯", 30)
        - sogou_search("Pythonç¼–ç¨‹æ•™ç¨‹", 20)
    """
    return await multi_engine.search(
        query=query,
        engine="sogou",
        num_results=num_results,
        search_type="web",
    )


@server.tool()
async def sogou_news_search(
    query: str,
    num_results: int = 30,
) -> str:
    """æœç‹—æ–°é—»æœç´¢ - ä½¿ç”¨æœç‹—æ–°é—»æœç´¢

    Args:
        query: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "äººå·¥æ™ºèƒ½"ã€"ç§‘æŠ€"ï¼‰
        num_results: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ 30ï¼Œå»ºè®® 20-50ï¼‰

    Returns:
        JSON æ ¼å¼çš„æ–°é—»æœç´¢ç»“æœ

    Examples:
        - sogou_news_search("ç§‘æŠ€", 30)
        - sogou_news_search("äººå·¥æ™ºèƒ½", 40)
    """
    return await multi_engine.search(
        query=query,
        engine="sogou",
        num_results=num_results,
        search_type="news",
    )


if __name__ == "__main__":
    logger.info("ğŸš€ ç™¾åº¦æœç´¢ MCP Server å¯åŠ¨")
    server.run()
