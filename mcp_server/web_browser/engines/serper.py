"""Serper.dev æœç´¢å¼•æ“ - ä½¿ç”¨ Google Search APIï¼ˆä¸ä¾èµ–æµè§ˆå™¨ï¼‰"""

import os
from dataclasses import dataclass
from typing import List, Optional

import httpx
from loguru import logger


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    title: str
    url: str
    summary: str = ""
    source: str = ""
    time: str = ""


@dataclass
class SerperConfig:
    """Serper é…ç½®"""
    api_key: str
    search_url: str = "https://google.serper.dev/search"
    news_url: str = "https://google.serper.dev/news"
    timeout: int = 30


class SerperEngine:
    """Serper.dev æœç´¢å¼•æ“ - ä½¿ç”¨ API è€Œéæµè§ˆå™¨

    ç‰¹ç‚¹ï¼š
    - ä¸éœ€è¦ Playwright æµè§ˆå™¨
    - ç›´æ¥é€šè¿‡ HTTP API è°ƒç”¨ Google æœç´¢
    - é€Ÿåº¦å¿«ï¼Œç¨³å®šæ€§é«˜
    - éœ€è¦ API Keyï¼ˆä» https://serper.dev è·å–ï¼‰
    """

    def __init__(self, api_key: Optional[str] = None):
        """åˆå§‹åŒ– Serper å¼•æ“

        Args:
            api_key: Serper API Keyï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        # å°è¯•ä»å¤šä¸ªæ¥æºè·å– API Key
        self.api_key = api_key or os.getenv("SERPER_API_KEY")

        if not self.api_key:
            logger.warning("âš ï¸ SERPER_API_KEY æœªé…ç½®ï¼ŒSerper æœç´¢å°†ä¸å¯ç”¨")

        self.name = "Serper"

    @property
    def engine_id(self) -> str:
        """å¼•æ“ID"""
        return "serper"

    def is_available(self) -> bool:
        """æ£€æŸ¥å¼•æ“æ˜¯å¦å¯ç”¨"""
        return bool(self.api_key)

    async def search(
        self,
        query: str,
        num_results: int = 30,
        search_type: str = "web",
    ) -> List[SearchResult]:
        """æ‰§è¡Œæœç´¢

        Args:
            query: æœç´¢å…³é”®è¯
            num_results: è¿”å›ç»“æœæ•°é‡ï¼ˆæœ€å¤§100ï¼‰
            search_type: æœç´¢ç±»å‹ ("web" æˆ– "news")

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.api_key:
            logger.error("âŒ SERPER_API_KEY æœªé…ç½®")
            return []

        # æ„å»ºè¯·æ±‚ URL
        url = self.config.news_url if search_type == "news" else self.config.search_url

        # æ„å»ºè¯·æ±‚å‚æ•°
        payload = {
            "q": query,
            "num": min(num_results, 100),  # Serper æœ€å¤§æ”¯æŒ 100
        }

        headers = {
            "X-API-KEY": self.api_key,
            "Content-Type": "application/json",
        }

        logger.info(f"   ğŸ” [Serper] {query} ({search_type})")

        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.post(url, json=payload, headers=headers)
                response.raise_for_status()
                data = response.json()

                # è§£æç»“æœ
                results = []

                # å¤„ç†æ™®é€šæœç´¢ç»“æœ
                if "organic" in data:
                    for item in data["organic"][:num_results]:
                        results.append(
                            SearchResult(
                                title=item.get("title", ""),
                                url=item.get("link", ""),
                                summary=item.get("snippet", ""),
                                source=self._extract_domain(item.get("link", "")),
                                time="",
                            )
                        )

                # å¤„ç†æ–°é—»ç»“æœ
                elif "news" in data:
                    for item in data["news"][:num_results]:
                        results.append(
                            SearchResult(
                                title=item.get("title", ""),
                                url=item.get("link", ""),
                                summary=item.get("snippet", ""),
                                source=item.get("source", ""),
                                time=item.get("date", ""),
                            )
                        )

                logger.info(f"   âœ… [Serper] æˆåŠŸè·å– {len(results)} æ¡ç»“æœ")
                return results

        except httpx.HTTPStatusError as e:
            if e.response.status_code == 401:
                logger.error("âŒ [Serper] API è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key")
            elif e.response.status_code == 429:
                logger.error("âŒ [Serper] API è¯·æ±‚è¶…é™ï¼Œè¯·ç¨åé‡è¯•")
            elif e.response.status_code == 403:
                logger.error("âŒ [Serper] API è®¿é—®è¢«æ‹’ç»ï¼Œå¯èƒ½éœ€è¦ä»˜è´¹è®¡åˆ’")
            else:
                logger.error(f"âŒ [Serper] API é”™è¯¯: {e.response.status_code}")
            return []

        except httpx.RequestError as e:
            logger.error(f"âŒ [Serper] è¯·æ±‚å¤±è´¥: {e}")
            return []

        except Exception as e:
            logger.error(f"âŒ [Serper] æœç´¢å¤±è´¥: {e}")
            return []

    @property
    def config(self) -> SerperConfig:
        """è·å–é…ç½®"""
        return SerperConfig(api_key=self.api_key or "")

    @staticmethod
    def _extract_domain(url: str) -> str:
        """ä» URL ä¸­æå–åŸŸå"""
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc or "unknown"
        except Exception:
            return "unknown"

    def extract_domain(self, url: str) -> str:
        """ä» URL ä¸­æå–åŸŸåï¼ˆå…¬å¼€æ–¹æ³•ï¼‰"""
        return self._extract_domain(url)

    def get_search_url(self, _query: str, _num_results: int, search_type: str = "web") -> str:
        """æ„å»ºæœç´¢ URLï¼ˆç”¨äºå…¼å®¹ï¼Œå®é™…ä¸ä½¿ç”¨ï¼‰"""
        return self.config.news_url if search_type == "news" else self.config.search_url
