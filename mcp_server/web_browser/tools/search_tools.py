"""æœç´¢å·¥å…· - ç»Ÿä¸€çš„æœç´¢æ¥å£"""

import json
import re
from typing import Optional

from loguru import logger

from ..config.settings import get_settings
from ..core.browser_pool import get_browser_pool
from ..core.rate_limiter import RateLimiter
from ..engines.base import SearchResult
from ..engines.factory import EngineFactory
from ..utils.helpers import get_random_user_agent, search_result_to_dict


# å…¨å±€å®ä¾‹
_settings = get_settings()
_browser_pool = get_browser_pool(_settings)
_rate_limiter = RateLimiter(
    time_window=_settings.rate_limit_time_window,
    max_domain_requests=_settings.max_domain_requests_per_second,
    max_engine_requests=_settings.max_engine_requests_per_second,
)
_engine_factory = EngineFactory(enabled_engines=_settings.enabled_engines)


async def _execute_search(
    engine_id: str,
    query: str,
    num_results: int = 30,
    search_type: str = "web",
) -> str:
    """æ‰§è¡Œæœç´¢çš„å†…éƒ¨å‡½æ•°"""
    engine = _engine_factory.get_engine(engine_id)
    if not engine:
        return json.dumps(
            {"error": f"æœç´¢å¼•æ“ {engine_id} ä¸å¯ç”¨"},
            ensure_ascii=False,
        )

    logger.info(f"ğŸ” [{engine.config.name}] {query} ({search_type})")

    # åº”ç”¨é€Ÿç‡é™åˆ¶
    search_url = engine.get_search_url(query, num_results, search_type)
    domain = engine.extract_domain(search_url)
    await _rate_limiter.acquire(domain=domain, engine=engine_id)

    try:
        user_agent = get_random_user_agent()
        async with _browser_pool.get_page(user_agent=user_agent) as page:
            results = await engine.search(page, query, num_results, search_type)

            results_dict = [search_result_to_dict(r) for r in results]

            return json.dumps(
                {
                    "engine": engine_id,
                    "engine_name": engine.config.name,
                    "query": query,
                    "total": len(results_dict),
                    "results": results_dict,
                },
                ensure_ascii=False,
                indent=2,
            )

    except Exception as e:
        logger.error(f"âŒ {engine.config.name} æœç´¢å¤±è´¥: {e}")
        return json.dumps(
            {
                "engine": engine_id,
                "engine_name": engine.config.name,
                "query": query,
                "total": 0,
                "results": [],
                "error": str(e),
            },
            ensure_ascii=False,
        )


async def _multi_search_with_fallback(
    query: str,
    preferred_engine: str = "auto",
    num_results: int = 30,
    search_type: str = "web",
) -> str:
    """å¤šæœç´¢å¼•æ“æœç´¢ï¼ˆå¸¦é™çº§ï¼‰"""
    # é€‰æ‹©å¼•æ“
    if preferred_engine == "auto":
        engine = _engine_factory.get_random_engine()
        engines_to_try = [engine] + _engine_factory.get_engines_by_priority()
    else:
        engine = _engine_factory.get_engine(preferred_engine)
        if not engine:
            engine = _engine_factory.get_random_engine()
        engines_to_try = [engine] + _engine_factory.get_engines_by_priority()

    # å»é‡
    seen_engines = set()
    unique_engines = []
    for e in engines_to_try:
        if e.engine_id not in seen_engines:
            seen_engines.add(e.engine_id)
            unique_engines.append(e)

    logger.info(f"   ğŸ“‹ å¼•æ“å°è¯•é¡ºåº: {[e.engine_id for e in unique_engines]}")

    # ä¾æ¬¡å°è¯•æ¯ä¸ªå¼•æ“
    for engine in unique_engines:
        try:
            result = await _execute_search(
                engine_id=engine.engine_id,
                query=query,
                num_results=num_results,
                search_type=search_type,
            )

            result_data = json.loads(result)
            if result_data.get("total", 0) > 0:
                return result

        except Exception as e:
            logger.warning(f"   âŒ {engine.config.name} æœç´¢å¤±è´¥: {e}")
            continue

    # æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥
    return json.dumps(
        {
            "query": query,
            "total": 0,
            "results": [],
            "error": "æ‰€æœ‰æœç´¢å¼•æ“å‡ä¸å¯ç”¨",
        },
        ensure_ascii=False,
    )


# ========== å…¬å¼€å·¥å…·å‡½æ•° ==========


async def baidu_search(
    query: str, num_results: int = 30, time_range: Optional[str] = None
) -> str:
    """ç™¾åº¦æœç´¢

    Args:
        query: æœç´¢å…³é”®è¯
        num_results: è¿”å›ç»“æœæ•°é‡
        time_range: æ—¶é—´èŒƒå›´ï¼ˆæš‚æœªå®ç°ï¼Œä¿ç•™å‚æ•°ï¼‰
    """
    _ = time_range  # ä¿ç•™å‚æ•°ï¼Œæš‚æœªå®ç°
    return await _execute_search("baidu", query, num_results, "web")


async def baidu_news_search(query: str, num_results: int = 30) -> str:
    """ç™¾åº¦æ–°é—»æœç´¢"""
    return await _execute_search("baidu", query, num_results, "news")


async def bing_search(query: str, num_results: int = 30) -> str:
    """å¿…åº”æœç´¢"""
    return await _execute_search("bing", query, num_results, "web")


async def bing_news_search(query: str, num_results: int = 30) -> str:
    """å¿…åº”æ–°é—»æœç´¢"""
    return await _execute_search("bing", query, num_results, "news")


async def sogou_search(query: str, num_results: int = 30) -> str:
    """æœç‹—æœç´¢"""
    return await _execute_search("sogou", query, num_results, "web")


async def sogou_news_search(query: str, num_results: int = 30) -> str:
    """æœç‹—æ–°é—»æœç´¢"""
    return await _execute_search("sogou", query, num_results, "news")


async def google_search(query: str, num_results: int = 30) -> str:
    """è°·æ­Œæœç´¢"""
    return await _execute_search("google", query, num_results, "web")


async def google_news_search(query: str, num_results: int = 30) -> str:
    """è°·æ­Œæ–°é—»æœç´¢"""
    return await _execute_search("google", query, num_results, "news")


async def search_360(query: str, num_results: int = 30) -> str:
    """360æœç´¢"""
    return await _execute_search("360", query, num_results, "web")


async def search_360_news(query: str, num_results: int = 30) -> str:
    """360æ–°é—»æœç´¢"""
    return await _execute_search("360", query, num_results, "news")


async def multi_search(
    query: str,
    engine: str = "auto",
    num_results: int = 30,
    search_type: str = "web",
) -> str:
    """å¤šæœç´¢å¼•æ“ - æ”¯æŒè‡ªåŠ¨é™çº§"""
    return await _multi_search_with_fallback(query, engine, num_results, search_type)


async def fetch_article_content(url: str) -> str:
    """è·å–æ–‡ç« æ­£æ–‡å†…å®¹"""
    logger.info(f"ğŸ“„ [è·å–æ–‡ç« æ­£æ–‡] URL: {url}")

    await _rate_limiter.acquire()

    try:
        user_agent = get_random_user_agent()
        async with _browser_pool.get_page(user_agent=user_agent) as page:
            await page.goto(url, timeout=30000)

            # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
            page_title = await page.title()
            if "éªŒè¯" in page_title or "å®‰å…¨" in page_title:
                logger.warning("   âš ï¸ è¢«å®‰å…¨éªŒè¯æ‹¦æˆª")
                return json.dumps(
                    {"url": url, "title": "", "content": "", "error": "è¢«å®‰å…¨éªŒè¯æ‹¦æˆª"},
                    ensure_ascii=False,
                )

            # æå–æ ‡é¢˜
            title = await _extract_title(page)

            # æå–æ­£æ–‡
            content = await _extract_content(page)

            # æ¸…ç†å†…å®¹
            if content:
                content = _clean_content(content)

            logger.info(f"âœ… æ–‡ç« å†…å®¹è·å–å®Œæˆï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")

            return json.dumps(
                {
                    "url": url,
                    "title": title,
                    "content": content,
                    "content_length": len(content),
                },
                ensure_ascii=False,
                indent=2,
            )

    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
        return json.dumps(
            {"url": url, "title": "", "content": "", "error": str(e)},
            ensure_ascii=False,
        )


async def _extract_title(page) -> str:
    """æå–æ–‡ç« æ ‡é¢˜"""
    title_selectors = [
        "h1",
        ".article-title",
        ".news-title",
        ".title",
        "[class*='title']",
        "#title",
    ]

    for selector in title_selectors:
        try:
            title_elem = await page.query_selector(selector)
            if title_elem:
                title_text = await title_elem.text_content()
                if title_text and len(title_text.strip()) > 5:
                    logger.info(f"   ğŸ“° æ ‡é¢˜: {title_text[:50]}...")
                    return title_text.strip()
        except Exception:
            continue

    return ""


async def _extract_content(page) -> str:
    """æå–æ–‡ç« æ­£æ–‡"""
    content_selectors = [
        "article",
        ".article-content",
        ".news-content",
        ".content",
        "[class*='content']",
        "#content",
        ".article-body",
        ".post-content",
        "main",
    ]

    for selector in content_selectors:
        try:
            content_elem = await page.query_selector(selector)
            if content_elem:
                paragraphs = await content_elem.query_selector_all("p")
                if paragraphs and len(paragraphs) >= 3:
                    content_parts = []
                    for p in paragraphs[:20]:
                        text = await p.text_content()
                        if text and len(text.strip()) > 10:
                            content_parts.append(text.strip())

                    if content_parts:
                        full_content = "\n\n".join(content_parts)
                        logger.info(f"   âœ… æå–åˆ° {len(content_parts)} ä¸ªæ®µè½")
                        return full_content
        except Exception:
            continue

    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ JavaScript æå–
    return await _extract_content_fallback(page)


async def _extract_content_fallback(page) -> str:
    """å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ JavaScript æå–å†…å®¹"""
    logger.warning("   âš ï¸ å¸¸è§„é€‰æ‹©å™¨å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")

    body_text = await page.evaluate(
        """() => {
        const clones = document.body.cloneNode(true);

        const unwantedSelectors = [
            'script', 'style', 'nav', 'header', 'footer', 'aside',
            'iframe', 'noscript', 'meta', 'link', '[class*="ad"]',
            '[class*="advertisement"]', '[class*="sidebar"]',
            '[class*="comment"]', '[class*="share"]', '[class*="social"]',
            '[id*="ad"]', '[id*="advertisement"]'
        ];

        unwantedSelectors.forEach(selector => {
            const elements = clones.querySelectorAll(selector);
            elements.forEach(el => el.remove());
        });

        const contentElements = clones.querySelectorAll('p, h1, h2, h3, h4, div, span');
        const texts = [];

        contentElements.forEach(el => {
            const text = el.textContent || el.innerText || '';
            const trimmed = text.trim();

            if (trimmed.length > 20 &&
                !trimmed.includes('ç‚¹å‡»') &&
                !trimmed.includes('å…³æ³¨') &&
                !trimmed.includes('è®¢é˜…') &&
                !trimmed.match(/^\\d+$/)) {
                texts.push(trimmed);
            }
        });

        const uniqueTexts = [...new Set(texts)];

        if (uniqueTexts.length >= 3) {
            return uniqueTexts.slice(0, 30).join('\\n\\n');
        }
        return '';
    }"""
    )

    if body_text and len(body_text) > 100:
        logger.info(f"   âœ… å¤‡ç”¨æ–¹æ¡ˆæå–åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(body_text)}")
        return body_text

    return ""


def _clean_content(content: str) -> str:
    """æ¸…ç†å’Œè§„èŒƒåŒ–å†…å®¹"""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    content = re.sub(r"\n{3,}", "\n\n", content)
    content = re.sub(r"[ \t]+", " ", content)
    content = re.sub(r"\n +", "\n", content)
    content = content.strip()

    # ç§»é™¤å¸¸è§çš„æ— ç”¨æ–‡æœ¬
    useless_patterns = [
        r"ç‚¹å‡»æŸ¥çœ‹.*è¯¦æƒ…",
        r"æ›´å¤šå†…å®¹è¯·.*",
        r"è´£ä»»ç¼–è¾‘.*",
        r"ç‰ˆæƒå£°æ˜.*",
        r"æœ¬æ–‡æ¥æº.*",
        r"è½¬è½½è¯·æ³¨æ˜.*",
        r"å…è´£å£°æ˜.*",
        r"å¹¿å‘Š.*",
    ]
    for pattern in useless_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    return content


async def baidu_hot_search() -> str:
    """è·å–ç™¾åº¦çƒ­æœæ¦œ"""
    logger.info("ğŸ”¥ [ç™¾åº¦çƒ­æœæ¦œ] è·å–çƒ­æœæ¦œå•")

    await _rate_limiter.acquire()

    try:
        hot_url = "https://top.baidu.com/board?tab=realtime"

        user_agent = get_random_user_agent()
        async with _browser_pool.get_page(user_agent=user_agent) as page:
            await page.goto(hot_url, timeout=30000)

            hot_items = await page.evaluate(
                """() => {
                const items = [];
                const elements = document.querySelectorAll('.category-wrap_iQLoo.horizontal_1eKyQ');

                elements.forEach((item, idx) => {
                    try {
                        const titleElem = item.querySelector('.c-single-text-ellipsis');
                        const title = titleElem ? titleElem.innerText?.trim() || '' : '';

                        const hotScoreElem = item.querySelector('.hot-index_1Bl1a');
                        const hotScore = hotScoreElem ? hotScoreElem.innerText?.trim() || '' : '';

                        const linkElem = item.querySelector('a');
                        const url = linkElem ? linkElem.getAttribute('href') || '' : '';

                        if (title) {
                            items.push({
                                rank: idx + 1,
                                title,
                                hot_score: hotScore,
                                url
                            });
                        }
                    } catch (e) {
                        // å¿½ç•¥å•ä¸ªæ¡ç›®çš„è§£æé”™è¯¯
                    }
                });

                return items;
            }"""
            )

        logger.info(f"âœ… çƒ­æœæ¦œè·å–å®Œæˆ: {len(hot_items)} æ¡")

        return json.dumps(
            {"total": len(hot_items), "hot_items": hot_items},
            ensure_ascii=False,
            indent=2,
        )

    except Exception as e:
        logger.error(f"âŒ è·å–ç™¾åº¦çƒ­æœå¤±è´¥: {e}")
        return json.dumps(
            {"total": 0, "hot_items": [], "error": str(e)},
            ensure_ascii=False,
        )
