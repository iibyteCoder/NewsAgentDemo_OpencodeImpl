"""æœç´¢å·¥å…· - ç»Ÿä¸€çš„æœç´¢æ¥å£"""

import json
import re
from typing import Optional

from loguru import logger
from playwright.async_api import Page

from ..config.settings import get_settings
from ..core.browser_pool import get_browser_pool
from ..core.rate_limiter import RateLimiter
from ..engines.base import SearchResult
from ..engines.factory import EngineFactory
from ..utils.helpers import get_random_user_agent, search_result_to_dict


# å…¨å±€å®ä¾‹
_settings = get_settings()
_browser_pool = get_browser_pool(_settings)
_rate_limiter = RateLimiter(
    time_window=_settings.rate_limit_time_window,
    max_domain_requests=_settings.max_domain_requests_per_second,
    max_engine_requests=_settings.max_engine_requests_per_second,
)
_engine_factory = EngineFactory(enabled_engines=_settings.enabled_engines)


async def _check_anti_bot(page: Page, url: str) -> tuple[bool, str]:
    """æ£€æµ‹é¡µé¢æ˜¯å¦è¢«åçˆ¬è™«æ‹¦æˆª

    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
        url: é¡µé¢URL

    Returns:
        (æ˜¯å¦è¢«æ‹¦æˆª, æ‹¦æˆªåŸå› )
    """
    try:
        # 1. æ£€æŸ¥HTTPçŠ¶æ€
        response = await page.evaluate("() => ({ status: window.performance?.getEntriesByType?.('navigation')?.[0]?.responseStatus || 200 })")
        if response and response.get("status", 200) >= 400:
            return True, f"HTTPé”™è¯¯: {response['status']}"

        # 2. æ£€æŸ¥é¡µé¢æ ‡é¢˜
        page_title = await page.title()
        anti_bot_keywords = [
            "éªŒè¯", "å®‰å…¨", "captcha", "äººæœºéªŒè¯", "æœºå™¨äºº", "robot", "éªŒè¯ç ",
            "æ»‘åŠ¨éªŒè¯", "ç‚¹é€‰éªŒè¯", "çŸ­ä¿¡éªŒè¯", "é˜¿é‡Œäº‘", "äº‘ç›¾", "è…¾è®¯äº‘", "å¤©å¾¡",
            "è®¿é—®é¢‘ç¹", "è¯·æ±‚è¿‡äºé¢‘ç¹", "æ“ä½œè¿‡äºé¢‘ç¹", "ç³»ç»Ÿæ£€æµ‹", "å¼‚å¸¸è®¿é—®",
            "é£é™©æ£€æµ‹", "å®‰å…¨æ£€æµ‹", "ccæ”»å‡»", "é˜²åˆ·", "åçˆ¬"
        ]

        page_title_lower = page_title.lower()
        for keyword in anti_bot_keywords:
            if keyword.lower() in page_title_lower or keyword in page_title:
                return True, f"é¡µé¢æ ‡é¢˜åŒ…å«åçˆ¬è™«å…³é”®è¯: {keyword}"

        # 3. æ£€æŸ¥é¡µé¢å†…å®¹
        body_text = await page.evaluate("() => document.body.innerText?.substring(0, 500) || ''")
        anti_bot_phrases = [
            'è®¿é—®è¿‡äºé¢‘ç¹', 'è¯·æ±‚è¿‡äºé¢‘ç¹', 'æ“ä½œè¿‡äºé¢‘ç¹', 'ç³»ç»Ÿæ£€æµ‹åˆ°å¼‚å¸¸è®¿é—®',
            'ç–‘ä¼¼æœºå™¨äºº', 'äººæœºéªŒè¯', 'å®‰å…¨éªŒè¯', 'è¯·å®ŒæˆéªŒè¯', 'ipè¢«å°', 'ç¦æ­¢è®¿é—®',
            'access denied', 'forbidden', 'rate limit', 'too many requests'
        ]

        for phrase in anti_bot_phrases:
            if phrase.lower() in body_text.lower():
                return True, f"é¡µé¢å†…å®¹åŒ…å«åçˆ¬è™«æç¤º: {phrase}"

        # 4. æ£€æŸ¥éªŒè¯ç å…ƒç´ 
        captcha_elements = await page.evaluate("""() => {
            const selectors = ['#captcha', '.captcha', '[class*="captcha"]', '#geetest',
                             '[class*="geetest"]', '.recaptcha', '[class*="recaptcha"]',
                             '.verify', '[class*="verify"]'];
            for (const selector of selectors) {
                if (document.querySelector(selector)) {
                    return true;
                }
            }
            return false;
        }""")

        if captcha_elements:
            return True, "æ£€æµ‹åˆ°éªŒè¯ç å…ƒç´ "

        return False, ""

    except Exception as e:
        logger.warning(f"âš ï¸ åçˆ¬è™«æ£€æµ‹å¤±è´¥: {e}")
        return False, ""


async def _execute_search(
    engine_id: str,
    query: str,
    num_results: int = 30,
    search_type: str = "web",
) -> str:
    """æ‰§è¡Œæœç´¢çš„å†…éƒ¨å‡½æ•°ï¼ˆå¸¦åçˆ¬è™«æ£€æµ‹ï¼‰"""
    engine = _engine_factory.get_engine(engine_id)
    if not engine:
        return json.dumps(
            {"error": f"æœç´¢å¼•æ“ {engine_id} ä¸å¯ç”¨"},
            "engine": engine_id,
            "engine_name": engine_id,
            "query": query,
            "total": 0,
            "results": [],
            ensure_ascii=False,
        )

    logger.info(f"ğŸ” [{engine.config.name}] {query} ({search_type})")

    # åº”ç”¨é€Ÿç‡é™åˆ¶
    search_url = engine.get_search_url(query, num_results, search_type)
    domain = engine.extract_domain(search_url)
    await _rate_limiter.acquire(domain=domain, engine=engine_id)

    try:
        user_agent = get_random_user_agent()
        async with _browser_pool.get_page(user_agent=user_agent) as page:
            # å…ˆè®¿é—®é¡µé¢
            await page.goto(search_url, timeout=30000)

            # æ£€æµ‹åçˆ¬è™«æ‹¦æˆª
            is_blocked, block_reason = await _check_anti_bot(page, search_url)
            if is_blocked:
                logger.error(f"ğŸš¨ {engine.config.name} è¢«åçˆ¬è™«æ‹¦æˆª: {block_reason}")
                # ç¦ç”¨è¯¥å¼•æ“
                _engine_factory.ban_engine(engine_id, block_reason)
                return json.dumps(
                    {
                        "engine": engine_id,
                        "engine_name": engine.config.name,
                        "query": query,
                        "total": 0,
                        "results": [],
                        "blocked": True,
                        "block_reason": block_reason,
                        "error": "è¢«åçˆ¬è™«æ‹¦æˆª",
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            # æ‰§è¡Œæœç´¢
            results = await engine.search(page, query, num_results, search_type)

            # å¦‚æœæ²¡æœ‰ç»“æœï¼Œå¯èƒ½æ˜¯è¢«æ‹¦æˆªäº†
            if len(results) == 0:
                logger.warning(f"âš ï¸ {engine.config.name} è¿”å›0æ¡ç»“æœï¼Œå¯èƒ½è¢«æ‹¦æˆª")
                # ä¸ç¦ç”¨å¼•æ“ï¼Œåªè®°å½•è­¦å‘Š
                # å¦‚æœè¿ç»­å¤šæ¬¡å¤±è´¥ï¼Œå¯ä»¥è€ƒè™‘ç¦ç”¨

            results_dict = [search_result_to_dict(r) for r in results]

            return json.dumps(
                {
                    "engine": engine_id,
                    "engine_name": engine.config.name,
                    "query": query,
                    "total": len(results_dict),
                    "results": results_dict,
                },
                ensure_ascii=False,
                indent=2,
            )

    except Exception as e:
        logger.error(f"âŒ {engine.config.name} æœç´¢å¤±è´¥: {e}")
        return json.dumps(
            {
                "engine": engine_id,
                "engine_name": engine.config.name,
                "query": query,
                "total": 0,
                "results": [],
                "error": str(e),
            },
            ensure_ascii=False,
        )


async def _multi_search_with_fallback(
    query: str,
    preferred_engine: str = "auto",
    num_results: int = 30,
    search_type: str = "web",
) -> str:
    """å¤šæœç´¢å¼•æ“æœç´¢ï¼ˆæ™ºèƒ½é™çº§ï¼Œè‡ªåŠ¨è·³è¿‡è¢«ç¦ç”¨çš„å¼•æ“ï¼‰"""
    available_count = _engine_factory.get_available_engine_count()
    banned_count = _engine_factory.get_banned_engine_count()

    logger.info(f"ğŸ“Š å¯ç”¨å¼•æ“: {available_count}ä¸ª, è¢«ç¦ç”¨: {banned_count}ä¸ª")

    # é€‰æ‹©å¼•æ“
    if preferred_engine == "auto":
        engine = _engine_factory.get_random_engine()
        if not engine:
            return json.dumps(
                {
                    "query": query,
                    "total": 0,
                    "results": [],
                    "error": f"æ‰€æœ‰æœç´¢å¼•æ“å‡è¢«ç¦ç”¨ï¼Œè¯·ç¨åé‡è¯•ï¼ˆè¢«ç¦ç”¨å¼•æ“å°†åœ¨{EngineFactory.BAN_DURATION}ç§’åè‡ªåŠ¨è§£ç¦ï¼‰",
                },
                ensure_ascii=False,
            )
        engines_to_try = [engine] + _engine_factory.get_engines_by_priority()
    else:
        engine = _engine_factory.get_engine(preferred_engine)
        if not engine:
            engine = _engine_factory.get_random_engine()
        if not engine:
            return json.dumps(
                {
                    "query": query,
                    "total": 0,
                    "results": [],
                    "error": "æ‰€æœ‰æœç´¢å¼•æ“å‡è¢«ç¦ç”¨",
                },
                ensure_ascii=False,
            )
        engines_to_try = [engine] + _engine_factory.get_engines_by_priority()

    # å»é‡
    seen_engines = set()
    unique_engines = []
    for e in engines_to_try:
        if e and e.engine_id not in seen_engines:
            seen_engines.add(e.engine_id)
            unique_engines.append(e)

    logger.info(f"   ğŸ“‹ å¼•æ“å°è¯•é¡ºåº: {[e.engine_id for e in unique_engines]}")

    # ä¾æ¬¡å°è¯•æ¯ä¸ªå¼•æ“
    for engine in unique_engines:
        try:
            result = await _execute_search(
                engine_id=engine.engine_id,
                query=query,
                num_results=num_results,
                search_type=search_type,
            )

            result_data = json.loads(result)

            # å¦‚æœè¢«æ‹¦æˆªï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªå¼•æ“
            if result_data.get("blocked"):
                logger.warning(f"   âš ï¸ {engine.config.name} è¢«æ‹¦æˆªï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¼•æ“")
                continue

            # å¦‚æœæœ‰ç»“æœï¼Œè¿”å›
            if result_data.get("total", 0) > 0:
                logger.info(f"   âœ… {engine.config.name} æˆåŠŸè¿”å› {result_data['total']} æ¡ç»“æœ")
                # æ·»åŠ å¼•æ“çŠ¶æ€ä¿¡æ¯
                result_data["available_engines"] = available_count
                result_data["banned_engines"] = banned_count
                return result

        except Exception as e:
            logger.warning(f"   âŒ {engine.config.name} æœç´¢å¤±è´¥: {e}")
            continue

    # æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥
    return json.dumps(
        {
            "query": query,
            "total": 0,
            "results": [],
            "error": "æ‰€æœ‰æœç´¢å¼•æ“å‡ä¸å¯ç”¨æˆ–è¿”å›0æ¡ç»“æœ",
            "available_engines": _engine_factory.get_available_engine_count(),
            "banned_engines": _engine_factory.get_banned_engine_count(),
        },
        ensure_ascii=False,
    )


# ========== å…¬å¼€å·¥å…·å‡½æ•° ==========


async def baidu_search(
    query: str, num_results: int = 30, time_range: Optional[str] = None
) -> str:
    """ç™¾åº¦æœç´¢

    Args:
        query: æœç´¢å…³é”®è¯
        num_results: è¿”å›ç»“æœæ•°é‡
        time_range: æ—¶é—´èŒƒå›´ï¼ˆæš‚æœªå®ç°ï¼Œä¿ç•™å‚æ•°ï¼‰
    """
    _ = time_range  # ä¿ç•™å‚æ•°ï¼Œæš‚æœªå®ç°
    return await _execute_search("baidu", query, num_results, "web")


async def baidu_news_search(query: str, num_results: int = 30) -> str:
    """ç™¾åº¦æ–°é—»æœç´¢"""
    return await _execute_search("baidu", query, num_results, "news")


async def bing_search(query: str, num_results: int = 30) -> str:
    """å¿…åº”æœç´¢"""
    return await _execute_search("bing", query, num_results, "web")


async def bing_news_search(query: str, num_results: int = 30) -> str:
    """å¿…åº”æ–°é—»æœç´¢"""
    return await _execute_search("bing", query, num_results, "news")


async def sogou_search(query: str, num_results: int = 30) -> str:
    """æœç‹—æœç´¢"""
    return await _execute_search("sogou", query, num_results, "web")


async def sogou_news_search(query: str, num_results: int = 30) -> str:
    """æœç‹—æ–°é—»æœç´¢"""
    return await _execute_search("sogou", query, num_results, "news")


async def google_search(query: str, num_results: int = 30) -> str:
    """è°·æ­Œæœç´¢"""
    return await _execute_search("google", query, num_results, "web")


async def google_news_search(query: str, num_results: int = 30) -> str:
    """è°·æ­Œæ–°é—»æœç´¢"""
    return await _execute_search("google", query, num_results, "news")


async def search_360(query: str, num_results: int = 30) -> str:
    """360æœç´¢"""
    return await _execute_search("360", query, num_results, "web")


async def search_360_news(query: str, num_results: int = 30) -> str:
    """360æ–°é—»æœç´¢"""
    return await _execute_search("360", query, num_results, "news")


async def toutiao_search(query: str, num_results: int = 30) -> str:
    """ä»Šæ—¥å¤´æ¡æœç´¢"""
    return await _execute_search("toutiao", query, num_results, "web")


async def toutiao_news_search(query: str, num_results: int = 30) -> str:
    """ä»Šæ—¥å¤´æ¡æ–°é—»æœç´¢"""
    return await _execute_search("toutiao", query, num_results, "news")


async def tencent_search(query: str, num_results: int = 30) -> str:
    """è…¾è®¯æ–°é—»æœç´¢"""
    return await _execute_search("tencent", query, num_results, "web")


async def tencent_news_search(query: str, num_results: int = 30) -> str:
    """è…¾è®¯æ–°é—»æœç´¢"""
    return await _execute_search("tencent", query, num_results, "news")


async def wangyi_search(query: str, num_results: int = 30) -> str:
    """ç½‘æ˜“æ–°é—»æœç´¢"""
    return await _execute_search("wangyi", query, num_results, "web")


async def wangyi_news_search(query: str, num_results: int = 30) -> str:
    """ç½‘æ˜“æ–°é—»æœç´¢"""
    return await _execute_search("wangyi", query, num_results, "news")


async def sina_search(query: str, num_results: int = 30) -> str:
    """æ–°æµªæ–°é—»æœç´¢"""
    return await _execute_search("sina", query, num_results, "web")


async def sina_news_search(query: str, num_results: int = 30) -> str:
    """æ–°æµªæ–°é—»æœç´¢"""
    return await _execute_search("sina", query, num_results, "news")


async def sohu_search(query: str, num_results: int = 30) -> str:
    """æœç‹æ–°é—»æœç´¢"""
    return await _execute_search("sohu", query, num_results, "web")


async def sohu_news_search(query: str, num_results: int = 30) -> str:
    """æœç‹æ–°é—»æœç´¢"""
    return await _execute_search("sohu", query, num_results, "news")


async def multi_search(
    query: str,
    engine: str = "auto",
    num_results: int = 30,
    search_type: str = "web",
) -> str:
    """å¤šæœç´¢å¼•æ“ - æ”¯æŒè‡ªåŠ¨é™çº§"""
    return await _multi_search_with_fallback(query, engine, num_results, search_type)


async def fetch_article_content(url: str, include_images: bool = True) -> str:
    """è·å–æ–‡ç« æ­£æ–‡å†…å®¹

    Args:
        url: æ–‡ç« URL
        include_images: æ˜¯å¦æå–å›¾ç‰‡é“¾æ¥ï¼ˆé»˜è®¤Trueï¼‰

    Note:
        å§‹ç»ˆä¼šæ£€æµ‹å¹¶è¿”å›é¡µé¢çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
        - HTTPçŠ¶æ€ç 
        - é¡µé¢åŠ è½½çŠ¶æ€
        - å†…å®¹è´¨é‡è¯„ä¼°
        - æ™ºèƒ½å»ºè®®
    """
    logger.info(f"ğŸ“„ [è·å–æ–‡ç« æ­£æ–‡] URL: {url}")

    await _rate_limiter.acquire()

    try:
        user_agent = get_random_user_agent()
        async with _browser_pool.get_page(user_agent=user_agent) as page:
            response = await page.goto(url, timeout=30000)

            # å§‹ç»ˆæ£€æŸ¥é¡µé¢çŠ¶æ€
            status = await _check_page_status(page, response, url)

            # å¦‚æœé¡µé¢çŠ¶æ€å¼‚å¸¸ï¼Œç›´æ¥è¿”å›çŠ¶æ€ä¿¡æ¯
            if status.get("status") == "error":
                logger.warning(f"   âš ï¸ é¡µé¢å¼‚å¸¸: {status.get('reason')}")
                return json.dumps(
                    {
                        "url": url,
                        "status": status,
                        "title": "",
                        "content": "",
                        "images": [],
                        "suggestions": status.get("suggestions", []),
                    },
                    ensure_ascii=False,
                    indent=2,
                )

            logger.info(f"   âœ“ é¡µé¢çŠ¶æ€: {status.get('status', 'unknown')}")

            # æå–æ ‡é¢˜
            title = await _extract_title(page)

            # æå–æ­£æ–‡
            content = await _extract_content(page)

            # æ¸…ç†å†…å®¹
            if content:
                content = _clean_content(content)

            # å§‹ç»ˆæ£€æŸ¥å†…å®¹è´¨é‡
            if content:
                content_quality = _assess_content_quality(content, title, len(content))
                status.update(content_quality)

            # æå–å›¾ç‰‡é“¾æ¥
            images = []
            if include_images:
                images = await _extract_images(page, url)
                logger.info(f"   ğŸ–¼ï¸ æå–åˆ° {len(images)} ä¸ªå›¾ç‰‡é“¾æ¥")

            logger.info(f"âœ… æ–‡ç« å†…å®¹è·å–å®Œæˆï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")

            # æ„å»ºç»“æœï¼Œå§‹ç»ˆåŒ…å«çŠ¶æ€ä¿¡æ¯
            result = {
                "url": url,
                "title": title,
                "content": content,
                "content_length": len(content),
                "images": images,
                "image_count": len(images),
                "status": status,
            }

            # æ ¹æ®çŠ¶æ€ç»™å‡ºå»ºè®®
            if status.get("status") in ["warning", "poor"]:
                result["suggestions"] = _get_suggestions(status)
            elif status.get("status") == "ok":
                result["suggestions"] = ["âœ… é¡µé¢çŠ¶æ€æ­£å¸¸"]

            return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        logger.error(f"âŒ è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")
        error_status = {
            "status": "error",
            "reason": f"è¯·æ±‚å¤±è´¥: {str(e)}",
            "error_type": type(e).__name__,
        }
        return json.dumps(
            {
                "url": url,
                "status": error_status,
                "title": "",
                "content": "",
                "images": [],
                "suggestions": ["æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®", "å°è¯•ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“"],
            },
            ensure_ascii=False,
            indent=2,
        )


async def _extract_title(page) -> str:
    """æå–æ–‡ç« æ ‡é¢˜"""
    title_selectors = [
        "h1",
        ".article-title",
        ".news-title",
        ".title",
        "[class*='title']",
        "#title",
    ]

    for selector in title_selectors:
        try:
            title_elem = await page.query_selector(selector)
            if title_elem:
                title_text = await title_elem.text_content()
                if title_text and len(title_text.strip()) > 5:
                    logger.info(f"   ğŸ“° æ ‡é¢˜: {title_text[:50]}...")
                    return title_text.strip()
        except Exception:
            continue

    return ""


async def _extract_content(page) -> str:
    """æå–æ–‡ç« æ­£æ–‡"""
    content_selectors = [
        "article",
        ".article-content",
        ".news-content",
        ".content",
        "[class*='content']",
        "#content",
        ".article-body",
        ".post-content",
        "main",
    ]

    for selector in content_selectors:
        try:
            content_elem = await page.query_selector(selector)
            if content_elem:
                paragraphs = await content_elem.query_selector_all("p")
                if paragraphs and len(paragraphs) >= 3:
                    content_parts = []
                    for p in paragraphs[:20]:
                        text = await p.text_content()
                        if text and len(text.strip()) > 10:
                            content_parts.append(text.strip())

                    if content_parts:
                        full_content = "\n\n".join(content_parts)
                        logger.info(f"   âœ… æå–åˆ° {len(content_parts)} ä¸ªæ®µè½")
                        return full_content
        except Exception:
            continue

    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ JavaScript æå–
    return await _extract_content_fallback(page)


async def _extract_content_fallback(page) -> str:
    """å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ JavaScript æå–å†…å®¹"""
    logger.warning("   âš ï¸ å¸¸è§„é€‰æ‹©å™¨å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ")

    body_text = await page.evaluate(
        """() => {
        const clones = document.body.cloneNode(true);

        const unwantedSelectors = [
            'script', 'style', 'nav', 'header', 'footer', 'aside',
            'iframe', 'noscript', 'meta', 'link', '[class*="ad"]',
            '[class*="advertisement"]', '[class*="sidebar"]',
            '[class*="comment"]', '[class*="share"]', '[class*="social"]',
            '[id*="ad"]', '[id*="advertisement"]'
        ];

        unwantedSelectors.forEach(selector => {
            const elements = clones.querySelectorAll(selector);
            elements.forEach(el => el.remove());
        });

        const contentElements = clones.querySelectorAll('p, h1, h2, h3, h4, div, span');
        const texts = [];

        contentElements.forEach(el => {
            const text = el.textContent || el.innerText || '';
            const trimmed = text.trim();

            if (trimmed.length > 20 &&
                !trimmed.includes('ç‚¹å‡»') &&
                !trimmed.includes('å…³æ³¨') &&
                !trimmed.includes('è®¢é˜…') &&
                !trimmed.match(/^\\d+$/)) {
                texts.push(trimmed);
            }
        });

        const uniqueTexts = [...new Set(texts)];

        if (uniqueTexts.length >= 3) {
            return uniqueTexts.slice(0, 30).join('\\n\\n');
        }
        return '';
    }"""
    )

    if body_text and len(body_text) > 100:
        logger.info(f"   âœ… å¤‡ç”¨æ–¹æ¡ˆæå–åˆ°å†…å®¹ï¼Œé•¿åº¦: {len(body_text)}")
        return body_text

    return ""


def _clean_content(content: str) -> str:
    """æ¸…ç†å’Œè§„èŒƒåŒ–å†…å®¹"""
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    content = re.sub(r"\n{3,}", "\n\n", content)
    content = re.sub(r"[ \t]+", " ", content)
    content = re.sub(r"\n +", "\n", content)
    content = content.strip()

    # ç§»é™¤å¸¸è§çš„æ— ç”¨æ–‡æœ¬
    useless_patterns = [
        r"ç‚¹å‡»æŸ¥çœ‹.*è¯¦æƒ…",
        r"æ›´å¤šå†…å®¹è¯·.*",
        r"è´£ä»»ç¼–è¾‘.*",
        r"ç‰ˆæƒå£°æ˜.*",
        r"æœ¬æ–‡æ¥æº.*",
        r"è½¬è½½è¯·æ³¨æ˜.*",
        r"å…è´£å£°æ˜.*",
        r"å¹¿å‘Š.*",
    ]
    for pattern in useless_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    return content


async def _extract_images(page, base_url: str) -> list[dict]:
    """æå–æ–‡ç« ä¸­çš„å›¾ç‰‡é“¾æ¥

    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
        base_url: åŸºç¡€URLï¼ˆç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰

    Returns:
        å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå›¾ç‰‡åŒ…å« url, alt, width, height
    """
    try:
        images = await page.evaluate(
            """(baseUrl) => {
            const images = [];
            const imgElements = document.querySelectorAll('article img, .content img, .article-content img, main img, .news-content img, [class*="content"] img');

            imgElements.forEach((img, idx) => {
                const src = img.src || img.getAttribute('data-src');
                if (src) {
                    // å¤„ç†ç›¸å¯¹è·¯å¾„
                    let fullUrl = src;
                    if (src.startsWith('//')) {
                        fullUrl = 'https:' + src;
                    } else if (src.startsWith('/')) {
                        try {
                            const urlObj = new URL(baseUrl);
                            fullUrl = urlObj.origin + src;
                        } catch (e) {
                            fullUrl = src;
                        }
                    } else if (!src.startsWith('http')) {
                        try {
                            fullUrl = new URL(src, baseUrl).href;
                        } catch (e) {
                            fullUrl = src;
                        }
                    }

                    images.push({
                        index: idx + 1,
                        url: fullUrl,
                        alt: img.alt || '',
                        title: img.title || '',
                        width: img.naturalWidth || img.width || 0,
                        height: img.naturalHeight || img.height || 0
                    });
                }
            });

            return images;
        }""",
            base_url,
        )

        logger.info(f"   ğŸ–¼ï¸ æ‰¾åˆ° {len(images)} ä¸ªå›¾ç‰‡")
        return images

    except Exception as e:
        logger.warning(f"   âš ï¸ æå–å›¾ç‰‡å¤±è´¥: {e}")
        return []


async def _check_page_status(page, response, url: str) -> dict:
    """æ£€æŸ¥é¡µé¢çŠ¶æ€ï¼ˆåŠ å¼ºåçˆ¬è™«æ£€æµ‹ï¼‰

    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
        response: å“åº”å¯¹è±¡
        url: é¡µé¢URL

    Returns:
        çŠ¶æ€ä¿¡æ¯å­—å…¸
    """
    status_info = {
        "status": "unknown",
        "checks": [],
        "anti_bot_detected": False,  # åçˆ¬è™«æ£€æµ‹æ ‡è®°
    }

    try:
        # 1. æ£€æŸ¥HTTPçŠ¶æ€ç 
        if response:
            status_code = response.status
            status_info["http_status"] = status_code

            if status_code >= 400:
                status_info["status"] = "error"
                status_info["reason"] = f"HTTPé”™è¯¯: {status_code}"
                status_info["checks"].append(f"HTTPçŠ¶æ€ç å¼‚å¸¸: {status_code}")

                if status_code == 404:
                    status_info["suggestions"] = ["é¡µé¢ä¸å­˜åœ¨", "æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®", "å°è¯•æœç´¢ç›¸å…³å†…å®¹"]
                elif status_code == 403:
                    status_info["suggestions"] = ["è®¿é—®è¢«æ‹’ç»", "å¯èƒ½éœ€è¦ç™»å½•", "å°è¯•ä½¿ç”¨å…¶ä»–ç½‘ç«™"]
                elif status_code >= 500:
                    status_info["suggestions"] = ["æœåŠ¡å™¨é”™è¯¯", "ç¨åé‡è¯•", "å°è¯•ä½¿ç”¨é•œåƒç½‘ç«™"]

                return status_info

            status_info["checks"].append(f"HTTPçŠ¶æ€ç æ­£å¸¸: {status_code}")

        # 2. æ£€æŸ¥é¡µé¢æ ‡é¢˜ï¼ˆåŠ å¼ºåçˆ¬è™«æ£€æµ‹ï¼‰
        page_title = await page.title()
        status_info["page_title"] = page_title

        # åçˆ¬è™«å…³é”®è¯åˆ—è¡¨ï¼ˆæ‰©å±•ï¼‰
        anti_bot_keywords = [
            "éªŒè¯",
            "å®‰å…¨",
            "captcha",
            "äººæœºéªŒè¯",
            "æœºå™¨äºº",
            "robot",
            "bot",
            "éªŒè¯ç ",
            "æ»‘åŠ¨éªŒè¯",
            "ç‚¹é€‰éªŒè¯",
            "çŸ­ä¿¡éªŒè¯",
            "é˜¿é‡Œäº‘",
            "äº‘ç›¾",
            "è…¾è®¯äº‘",
            "å¤©å¾¡",
            "è®¿é—®é¢‘ç¹",
            "è¯·æ±‚è¿‡äºé¢‘ç¹",
            "æ“ä½œè¿‡äºé¢‘ç¹",
            "ç³»ç»Ÿæ£€æµ‹",
            "å¼‚å¸¸è®¿é—®",
            "é£é™©æ£€æµ‹",
            "å®‰å…¨æ£€æµ‹",
            "ccæ”»å‡»",
            "é˜²åˆ·",
            "åçˆ¬",
        ]

        page_title_lower = page_title.lower()
        for keyword in anti_bot_keywords:
            if keyword.lower() in page_title_lower or keyword in page_title:
                status_info["status"] = "error"
                status_info["reason"] = f"è¢«åçˆ¬è™«æ‹¦æˆª: æ£€æµ‹åˆ°å…³é”®è¯ '{keyword}'"
                status_info["anti_bot_detected"] = True
                status_info["anti_bot_type"] = "title_keyword"
                status_info["checks"].append(f"æ ‡é¢˜åŒ…å«åçˆ¬è™«å…³é”®è¯: {keyword}")
                status_info["suggestions"] = [
                    "âŒ è¢«åçˆ¬è™«éªŒè¯æ‹¦æˆª",
                    "ğŸš« æ£€æµ‹åˆ°åçˆ¬è™«å…³é”®è¯ï¼Œå»ºè®®æš‚åœä½¿ç”¨",
                    "â° ç­‰å¾…è¾ƒé•¿æ—¶é—´åé‡è¯•ï¼ˆå»ºè®®30åˆ†é’Ÿä»¥ä¸Šï¼‰",
                    "ğŸ”„ è€ƒè™‘æ›´æ¢IPæˆ–ä½¿ç”¨ä»£ç†",
                    "ğŸ” å°è¯•ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“",
                    "ğŸ“± å°è¯•ä½¿ç”¨ç§»åŠ¨ç«¯ç½‘ç«™",
                ]
                logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°åçˆ¬è™«æ‹¦æˆªï¼ˆæ ‡é¢˜ï¼‰: {keyword}")
                return status_info

        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
        error_keywords = ["404", "ä¸å­˜åœ¨", "æ— æ³•è®¿é—®", "not found", "é¡µé¢ä¸å­˜åœ¨", "è®¿é—®å¤±è´¥"]
        if any(keyword in page_title for keyword in error_keywords):
            status_info["status"] = "error"
            status_info["reason"] = "é¡µé¢ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®"
            status_info["checks"].append("æ ‡é¢˜åŒ…å«é”™è¯¯ä¿¡æ¯")
            status_info["suggestions"] = [
                "é¡µé¢ä¸å­˜åœ¨",
                "æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®",
                "å°è¯•æœç´¢å…¶ä»–æ¥æº",
            ]
            return status_info

        status_info["checks"].append("é¡µé¢æ ‡é¢˜æ­£å¸¸")

        # 3. ä½¿ç”¨JavaScriptæ£€æŸ¥é¡µé¢å†…å®¹ï¼ˆåŠ å¼ºåçˆ¬è™«æ£€æµ‹ï¼‰
        page_check = await page.evaluate(
            """() => {
            const checks = {
                hasBody: !!document.body,
                bodyText: document.body ? document.body.innerText.substring(0, 500) : '',
                hasArticle: !!document.querySelector('article'),
                hasContent: !!document.querySelector('.content, .article-content, main, [class*="content"]'),
                errorCode: null,
                needsLogin: false,
                isEmpty: false,
                // åçˆ¬è™«æ£€æµ‹
                hasCaptcha: false,
                captchaElements: [],
                antiBotElements: [],
                accessDenied: false,
                ipBlocked: false,
            };

            // æ£€æŸ¥éªŒè¯ç ç›¸å…³å…ƒç´ 
            const captchaSelectors = [
                '#captcha',
                '.captcha',
                '[class*="captcha"]',
                '[id*="captcha"]',
                '.geetest',
                '#geetest',
                '[class*="geetest"]',
                '.recaptcha',
                '[class*="recaptcha"]',
                '.verify',
                '[class*="verify"]',
                '.validate',
                '[class*="validate"]',
                'iframe[src*="captcha"]',
                'iframe[src*="verify"]',
            ];

            captchaSelectors.forEach(selector => {
                const elements = document.querySelectorAll(selector);
                if (elements.length > 0) {
                    checks.hasCaptcha = true;
                    checks.captchaElements.push(selector);
                }
            });

            // æ£€æŸ¥åçˆ¬è™«æç¤ºæ–‡æœ¬
            const bodyText = document.body.innerText.toLowerCase();
            const antiBotPhrases = [
                'è®¿é—®è¿‡äºé¢‘ç¹',
                'è¯·æ±‚è¿‡äºé¢‘ç¹',
                'æ“ä½œè¿‡äºé¢‘ç¹',
                'æ‚¨çš„è®¿é—®è¿‡äºé¢‘ç¹',
                'è¯·ç¨åå†è¯•',
                'ç³»ç»Ÿæ£€æµ‹åˆ°å¼‚å¸¸è®¿é—®',
                'ç–‘ä¼¼æœºå™¨äºº',
                'äººæœºéªŒè¯',
                'å®‰å…¨éªŒè¯',
                'è¯·å®ŒæˆéªŒè¯',
                'æ»‘åŠ¨éªŒè¯',
                'ç‚¹é€‰éªŒè¯',
                'é˜¿é‡Œäº‘ç›¾',
                'è…¾è®¯äº‘å¤©å¾¡',
                'é£é™©æ§åˆ¶',
                'å®‰å…¨æ£€æµ‹',
                'ccé˜²å¾¡',
                'wafé˜²ç«å¢™',
                'è®¿é—®è¢«æ‹’ç»',
                'ipè¢«å°',
                'ç¦æ­¢è®¿é—®',
                'access denied',
                'forbidden',
                'blocked',
                'rate limit',
                'too many requests',
            ];

            antiBotPhrases.forEach(phrase => {
                if (bodyText.includes(phrase)) {
                    checks.antiBotElements.push(phrase);
                }
            });

            // æ£€æŸ¥æ˜¯å¦IPè¢«å°ç¦
            const ipBlockedPhrases = [
                'ipè¢«å°',
                'ipå·²è¢«å°',
                'ipç¦æ­¢',
                'ipé™åˆ¶',
                'å°ç¦ip',
                'ç¦æ­¢ip',
                'blocked ip',
                'ip blocked',
            ];

            checks.ipBlocked = ipBlockedPhrases.some(phrase => bodyText.includes(phrase));

            // æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
            const loginKeywords = ['ç™»å½•', 'login', 'signin', 'è¯·å…ˆç™»å½•', 'éœ€è¦ç™»å½•'];
            checks.needsLogin = loginKeywords.some(keyword =>
                document.body.innerText.includes(keyword)
            );

            // æ£€æŸ¥é¡µé¢æ˜¯å¦ä¸ºç©º
            const textLength = document.body.innerText.trim().length;
            checks.isEmpty = textLength < 100;
            checks.textLength = textLength;

            return checks;
        }"""
        )

        status_info["page_checks"] = {
            "has_body": page_check["hasBody"],
            "has_article": page_check["hasArticle"],
            "has_content": page_check["hasContent"],
            "text_length": page_check.get("textLength", 0),
        }

        # æ£€æŸ¥éªŒè¯ç å…ƒç´ ï¼ˆé‡è¦ï¼ï¼‰
        if page_check.get("hasCaptcha") or page_check.get("captchaElements"):
            status_info["status"] = "error"
            status_info["reason"] = "è¢«åçˆ¬è™«æ‹¦æˆª: æ£€æµ‹åˆ°éªŒè¯ç "
            status_info["anti_bot_detected"] = True
            status_info["anti_bot_type"] = "captcha_element"
            status_info["checks"].append(f"æ£€æµ‹åˆ°éªŒè¯ç å…ƒç´ : {page_check.get('captchaElements', [])}")
            status_info["suggestions"] = [
                "âŒ è¢«åçˆ¬è™«éªŒè¯ç æ‹¦æˆª",
                "ğŸš« éœ€è¦äººå·¥éªŒè¯ï¼Œæµè§ˆå™¨å·²æ— æ³•ä½¿ç”¨",
                "â° å»ºè®®ç­‰å¾…è¾ƒé•¿æ—¶é—´åé‡è¯•ï¼ˆ30åˆ†é’Ÿä»¥ä¸Šï¼‰",
                "ğŸ”„ å¿…é¡»æ›´æ¢IPæˆ–ä½¿ç”¨ä»£ç†",
                "ğŸ” å°è¯•ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“",
                "ğŸ“± å°è¯•ä½¿ç”¨ç§»åŠ¨ç«¯ç½‘ç«™",
            ]
            logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°åçˆ¬è™«æ‹¦æˆªï¼ˆéªŒè¯ç ï¼‰: {page_check.get('captchaElements', [])}")
            return status_info

        # æ£€æŸ¥åçˆ¬è™«æç¤ºæ–‡æœ¬
        anti_bot_elements = page_check.get("antiBotElements", [])
        if anti_bot_elements:
            status_info["status"] = "error"
            status_info["reason"] = f"è¢«åçˆ¬è™«æ‹¦æˆª: {anti_bot_elements[0]}"
            status_info["anti_bot_detected"] = True
            status_info["anti_bot_type"] = "content_text"
            status_info["checks"].append(f"å†…å®¹åŒ…å«åçˆ¬è™«æ–‡æœ¬: {anti_bot_elements}")
            status_info["suggestions"] = [
                "âŒ è¢«åçˆ¬è™«æ‹¦æˆª",
                "ğŸš« æ£€æµ‹åˆ°åçˆ¬è™«æç¤ºï¼Œå»ºè®®æš‚åœä½¿ç”¨",
                "â° ç­‰å¾…è¾ƒé•¿æ—¶é—´åé‡è¯•ï¼ˆå»ºè®®30åˆ†é’Ÿä»¥ä¸Šï¼‰",
                "ğŸ”„ è€ƒè™‘æ›´æ¢IPæˆ–ä½¿ç”¨ä»£ç†",
                "ğŸ” å°è¯•ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“",
            ]
            logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°åçˆ¬è™«æ‹¦æˆªï¼ˆæ–‡æœ¬ï¼‰: {anti_bot_elements}")
            return status_info

        # æ£€æŸ¥IPæ˜¯å¦è¢«å°
        if page_check.get("ipBlocked"):
            status_info["status"] = "error"
            status_info["reason"] = "IPè¢«å°ç¦"
            status_info["anti_bot_detected"] = True
            status_info["anti_bot_type"] = "ip_blocked"
            status_info["checks"].append("æ£€æµ‹åˆ°IPå°ç¦æç¤º")
            status_info["suggestions"] = [
                "âŒ IPå·²è¢«å°ç¦",
                "ğŸš« å¿…é¡»æ›´æ¢IPæ‰èƒ½ç»§ç»­",
                "â° å»ºè®®ç­‰å¾…è¾ƒé•¿æ—¶é—´åé‡è¯•ï¼ˆ1å°æ—¶ä»¥ä¸Šï¼‰",
                "ğŸ”„ ä½¿ç”¨ä»£ç†æˆ–æ›´æ¢ç½‘ç»œ",
                "ğŸ” å°è¯•ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“",
            ]
            logger.warning("ğŸš¨ æ£€æµ‹åˆ°IPå°ç¦")
            return status_info

        # æ£€æŸ¥é”™è¯¯ä»£ç 
        if page_check.get("errorCode"):
            status_info["status"] = "error"
            status_info["reason"] = f"é¡µé¢è¿”å›é”™è¯¯: {page_check['errorCode']}"
            status_info["checks"].append(f"å†…å®¹åŒ…å«é”™è¯¯ä»£ç : {page_check['errorCode']}")
            status_info["suggestions"] = [
                "é¡µé¢æ— æ³•è®¿é—®",
                "å°è¯•ä½¿ç”¨å…¶ä»–ç½‘ç«™",
                "å°è¯•æœç´¢ç›¸å…³å†…å®¹",
            ]
            return status_info

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
        if page_check.get("needsLogin"):
            status_info["status"] = "warning"
            status_info["reason"] = "é¡µé¢å¯èƒ½éœ€è¦ç™»å½•"
            status_info["checks"].append("æ£€æµ‹åˆ°ç™»å½•æç¤º")
            status_info["suggestions"] = [
                "é¡µé¢éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®",
                "å°è¯•æœç´¢å…¬å¼€çš„å†…å®¹",
                "å¯»æ‰¾å…¶ä»–æ¥æº",
            ]
            return status_info

        # æ£€æŸ¥é¡µé¢æ˜¯å¦ä¸ºç©º
        if page_check.get("isEmpty"):
            status_info["status"] = "warning"
            status_info["reason"] = "é¡µé¢å†…å®¹è¿‡å°‘"
            status_info["checks"].append(f"é¡µé¢æ–‡æœ¬é•¿åº¦: {page_check.get('textLength', 0)}")
            status_info["suggestions"] = [
                "é¡µé¢å†…å®¹è¿‡å°‘",
                "å¯èƒ½æ˜¯åŠ è½½ä¸­æˆ–å†…å®¹è¢«é™åˆ¶",
                "å°è¯•ç­‰å¾…æˆ–ä½¿ç”¨å…¶ä»–æ¥æº",
            ]
            return status_info

        # æ‰€æœ‰æ£€æŸ¥é€šè¿‡
        status_info["status"] = "ok"
        status_info["reason"] = "é¡µé¢çŠ¶æ€æ­£å¸¸"
        status_info["checks"].append("é¡µé¢åŠ è½½æ­£å¸¸")
        status_info["anti_bot_detected"] = False

        return status_info

    except Exception as e:
        logger.error(f"æ£€æŸ¥é¡µé¢çŠ¶æ€å¤±è´¥: {e}")
        status_info["status"] = "error"
        status_info["reason"] = f"çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}"
        status_info["suggestions"] = ["æ— æ³•éªŒè¯é¡µé¢çŠ¶æ€", "å°è¯•ç›´æ¥è®¿é—®URL"]
        return status_info


def _assess_content_quality(content: str, title: str, content_length: int) -> dict:
    """è¯„ä¼°å†…å®¹è´¨é‡

    Args:
        content: æ–‡ç« å†…å®¹
        title: æ–‡ç« æ ‡é¢˜
        content_length: å†…å®¹é•¿åº¦

    Returns:
        è´¨é‡è¯„ä¼°ä¿¡æ¯
    """
    quality = {
        "quality": "unknown",
        "score": 0,
        "issues": [],
    }

    # 1. æ£€æŸ¥æ ‡é¢˜
    if not title or len(title) < 5:
        quality["issues"].append("æ ‡é¢˜è¿‡çŸ­æˆ–ç¼ºå¤±")
        quality["score"] -= 10
    else:
        quality["score"] += 10

    # 2. æ£€æŸ¥å†…å®¹é•¿åº¦
    if content_length < 100:
        quality["issues"].append("å†…å®¹è¿‡å°‘")
        quality["score"] -= 30
        quality["quality"] = "poor"
    elif content_length < 300:
        quality["issues"].append("å†…å®¹è¾ƒå°‘")
        quality["score"] -= 15
        quality["quality"] = "warning"
    elif content_length >= 500:
        quality["score"] += 20

    # 3. æ£€æŸ¥æ®µè½æ•°é‡
    paragraphs = content.split("\n\n")
    if len(paragraphs) < 2:
        quality["issues"].append("æ®µè½ç»“æ„ç®€å•")
        quality["score"] -= 10
    elif len(paragraphs) >= 5:
        quality["score"] += 10

    # 4. æ£€æŸ¥æ˜¯å¦åŒ…å«é”™è¯¯ä¿¡æ¯
    error_patterns = [
        "é¡µé¢ä¸å­˜åœ¨",
        "è®¿é—®å—é™",
        "è¯·ç™»å½•",
        "404",
        "403",
        "æ— æ³•è®¿é—®",
    ]
    for pattern in error_patterns:
        if pattern in content:
            quality["issues"].append(f"å†…å®¹åŒ…å«é”™è¯¯ä¿¡æ¯: {pattern}")
            quality["score"] -= 50
            quality["quality"] = "poor"
            break

    # 5. æ£€æŸ¥æ˜¯å¦åŒ…å«å¹¿å‘Šæˆ–æ— å…³å†…å®¹
    ad_patterns = ["å¹¿å‘Š", "ç‚¹å‡»æŸ¥çœ‹", "å…³æ³¨æˆ‘ä»¬", "æ‰«ç ", "åˆ†äº«"]
    ad_count = sum(1 for pattern in ad_patterns if pattern in content)
    if ad_count > 5:
        quality["issues"].append("å¯èƒ½åŒ…å«è¾ƒå¤šå¹¿å‘Šä¿¡æ¯")
        quality["score"] -= 5

    # ç¡®å®šè´¨é‡ç­‰çº§
    if quality["quality"] == "unknown":
        if quality["score"] >= 30:
            quality["quality"] = "good"
        elif quality["score"] >= 10:
            quality["quality"] = "acceptable"
        elif quality["score"] >= 0:
            quality["quality"] = "warning"
        else:
            quality["quality"] = "poor"

    return quality


def _get_suggestions(status: dict) -> list[str]:
    """æ ¹æ®çŠ¶æ€ç»™å‡ºå»ºè®®

    Args:
        status: çŠ¶æ€ä¿¡æ¯å­—å…¸

    Returns:
        å»ºè®®åˆ—è¡¨
    """
    suggestions = []

    status_level = status.get("status", "unknown")
    reason = status.get("reason", "")

    if status_level == "error":
        if "éªŒè¯" in reason or "captcha" in reason.lower():
            suggestions.extend(
                [
                    "âŒ è¢«åçˆ¬è™«éªŒè¯æ‹¦æˆª",
                    "ğŸ’¡ å»ºè®®ï¼šå°è¯•ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“",
                    "ğŸ’¡ å»ºè®®ï¼šç­‰å¾…å‡ ç§’åé‡è¯•",
                    "ğŸ’¡ å»ºè®®ï¼šå¯»æ‰¾å…¶ä»–ç½‘ç«™çš„ç›¸åŒå†…å®¹",
                ]
            )
        elif "404" in reason or "ä¸å­˜åœ¨" in reason:
            suggestions.extend(
                [
                    "âŒ é¡µé¢ä¸å­˜åœ¨",
                    "ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥URLæ˜¯å¦æ­£ç¡®",
                    "ğŸ’¡ å»ºè®®ï¼šå°è¯•æœç´¢ç›¸å…³å…³é”®è¯",
                    "ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“",
                ]
            )
        elif "403" in reason or "æ‹’ç»" in reason:
            suggestions.extend(
                [
                    "âŒ è®¿é—®è¢«æ‹’ç»",
                    "ğŸ’¡ å»ºè®®ï¼šå¯»æ‰¾å…¶ä»–å…¬å¼€æ¥æº",
                    "ğŸ’¡ å»ºè®®ï¼šå°è¯•ä½¿ç”¨æœç´¢å¼•æ“æ‰¾ç±»ä¼¼å†…å®¹",
                ]
            )
        else:
            suggestions.extend(
                [
                    f"âŒ {reason}",
                    "ğŸ’¡ å»ºè®®ï¼šå°è¯•ä½¿ç”¨å…¶ä»–æœç´¢å¼•æ“",
                    "ğŸ’¡ å»ºè®®ï¼šæœç´¢ç›¸å…³å…³é”®è¯",
                ]
            )

    elif status_level == "warning":
        if "ç™»å½•" in reason:
            suggestions.extend(
                [
                    "âš ï¸ é¡µé¢éœ€è¦ç™»å½•",
                    "ğŸ’¡ å»ºè®®ï¼šå¯»æ‰¾å…¬å¼€çš„å†…å®¹æ¥æº",
                    "ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨æœç´¢å¼•æ“æ‰¾ç›¸å…³æ–‡ç« ",
                ]
            )
        elif "è¿‡å°‘" in reason or "å†…å®¹" in reason:
            suggestions.extend(
                [
                    "âš ï¸ é¡µé¢å†…å®¹ä¸è¶³",
                    "ğŸ’¡ å»ºè®®ï¼šå°è¯•ä½¿ç”¨å…¶ä»–ç½‘ç«™",
                    "ğŸ’¡ å»ºè®®ï¼šæœç´¢æ›´å¤šç›¸å…³å†…å®¹",
                ]
            )
        else:
            suggestions.append(f"âš ï¸ {reason}")

    # æ ¹æ®è´¨é‡è¯„ä¼°ç»™å‡ºå»ºè®®
    quality = status.get("quality", "")
    if quality == "poor":
        suggestions.extend(
            [
                "ğŸ“Š å†…å®¹è´¨é‡è¯„ä¼°ï¼šè¾ƒå·®",
                "ğŸ’¡ å»ºè®®ï¼šå°è¯•å…¶ä»–æ¥æº",
                "ğŸ’¡ å»ºè®®ï¼šç»¼åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯",
            ]
        )
    elif quality == "warning":
        suggestions.extend(
            [
                "ğŸ“Š å†…å®¹è´¨é‡è¯„ä¼°ï¼šä¸€èˆ¬",
                "ğŸ’¡ å»ºè®®ï¼šå¯ä»¥å‚è€ƒï¼Œä½†å»ºè®®å¯»æ‰¾æ›´å¤šæ¥æº",
            ]
        )

    if not suggestions:
        suggestions.append("âœ… é¡µé¢çŠ¶æ€æ­£å¸¸")

    return suggestions


async def baidu_hot_search() -> str:
    """è·å–ç™¾åº¦çƒ­æœæ¦œ"""
    logger.info("ğŸ”¥ [ç™¾åº¦çƒ­æœæ¦œ] è·å–çƒ­æœæ¦œå•")

    await _rate_limiter.acquire()

    try:
        hot_url = "https://top.baidu.com/board?tab=realtime"

        user_agent = get_random_user_agent()
        async with _browser_pool.get_page(user_agent=user_agent) as page:
            await page.goto(hot_url, timeout=30000)

            hot_items = await page.evaluate(
                """() => {
                const items = [];
                const elements = document.querySelectorAll('.category-wrap_iQLoo.horizontal_1eKyQ');

                elements.forEach((item, idx) => {
                    try {
                        const titleElem = item.querySelector('.c-single-text-ellipsis');
                        const title = titleElem ? titleElem.innerText?.trim() || '' : '';

                        const hotScoreElem = item.querySelector('.hot-index_1Bl1a');
                        const hotScore = hotScoreElem ? hotScoreElem.innerText?.trim() || '' : '';

                        const linkElem = item.querySelector('a');
                        const url = linkElem ? linkElem.getAttribute('href') || '' : '';

                        if (title) {
                            items.push({
                                rank: idx + 1,
                                title,
                                hot_score: hotScore,
                                url
                            });
                        }
                    } catch (e) {
                        // å¿½ç•¥å•ä¸ªæ¡ç›®çš„è§£æé”™è¯¯
                    }
                });

                return items;
            }"""
            )

        logger.info(f"âœ… çƒ­æœæ¦œè·å–å®Œæˆ: {len(hot_items)} æ¡")

        return json.dumps(
            {"total": len(hot_items), "hot_items": hot_items},
            ensure_ascii=False,
            indent=2,
        )

    except Exception as e:
        logger.error(f"âŒ è·å–ç™¾åº¦çƒ­æœå¤±è´¥: {e}")
        return json.dumps(
            {"total": 0, "hot_items": [], "error": str(e)},
            ensure_ascii=False,
        )
